{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77d7d8b-e34c-4e35-b84c-b3e75066e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## comment\n",
    "###################################\n",
    "# few things to check -\n",
    "#         1. data generation => n, d, s0, graph_type, sem_type = 200, 10, 40, 'ER', 'mlp'\n",
    "#         2. env generation =>  noise_scales = [0.2, 1, 2, 5, 10]\n",
    "#         3. nhidden=10, lambda1=0.01, lambda2=0.01, w_threshold=0.3, std_lambda=1/10/100/1000, learning_rate=1e-3/1e-2, max_iter = 1000\n",
    "#         4. in utils.count_accuracy() => is_dag() is disabled/enabled?\n",
    "# few comment -\n",
    "#         1. acyclicity is important, we know the ground truth is DAG, not imposing DAGness is like not utilizing prior knowledge\n",
    "#         2. problem with acyclicity (notears) + invariance (VRex/IRM/CGLearn) is we do not know how or which way to make them compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676c5da2-0d91-42c0-bbb9-2f5c44c0e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## install and import\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63601855-7db7-49bf-b407-fec9a02b131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-igraph\n",
    "from notears.locally_connected import LocallyConnected\n",
    "from notears.lbfgsb_scipy import LBFGSBScipy\n",
    "from notears.trace_expm import trace_expm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import notears.utils as ut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7b4a60-2014-4cda-a316-5213198d6e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## class\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd09ec5-e518-4eae-892b-80a1da22818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotearsMLP(nn.Module):\n",
    "    def __init__(self, dims, bias=True):\n",
    "        super(NotearsMLP, self).__init__()\n",
    "        assert len(dims) >= 2\n",
    "        assert dims[-1] == 1\n",
    "        d = dims[0]\n",
    "        m1 = dims[1]        \n",
    "        self.dims = dims\n",
    "        # fc1: variable splitting for l1\n",
    "        ##\n",
    "        mask = torch.ones(d, m1, d)\n",
    "        for i in range(d):\n",
    "            for j in range(m1):\n",
    "                for k in range(d):\n",
    "                    if i==k:\n",
    "                        mask[i, j, k] = 0\n",
    "        mask = mask.view(d*m1, d)\n",
    "        self.register_buffer('diag_mask', mask)\n",
    "        ##\n",
    "        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "        # fc2: local linear layers\n",
    "        layers = []\n",
    "        for l in range(len(dims) - 2):\n",
    "            layers.append(LocallyConnected(d, dims[l + 1], dims[l + 2], bias=bias))\n",
    "        self.fc2 = nn.ModuleList(layers)\n",
    "\n",
    "    def _bounds(self):\n",
    "        d = self.dims[0]\n",
    "        bounds = []\n",
    "        for j in range(d):\n",
    "            for m in range(self.dims[1]):\n",
    "                for i in range(d):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0)\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        with torch.no_grad():\n",
    "            self.fc1_pos.weight *= self.diag_mask\n",
    "            self.fc1_neg.weight *= self.diag_mask\n",
    "            \n",
    "        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1]\n",
    "        x = x.view(-1, self.dims[0], self.dims[1])  # [n, d, m1]\n",
    "        for fc in self.fc2:\n",
    "            x = torch.sigmoid(x)  # [n, d, m1]\n",
    "            x = fc(x)  # [n, d, m2]\n",
    "        x = x.squeeze(dim=2)  # [n, d]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        \"\"\"Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(d) + A / d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, d - 1)\n",
    "        # h = (E.t() * M).sum() - d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        \"\"\"Take 2-norm-squared of all parameters\"\"\"\n",
    "        reg = 0.\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        reg += torch.sum(fc1_weight ** 2)\n",
    "        for fc in self.fc2:\n",
    "            reg += torch.sum(fc.weight ** 2)\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        \"\"\"Take l1 norm of fc1 weight\"\"\"\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n",
    "        \"\"\"Get W from fc1 weights, take 2-norm over m1 dim\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W\n",
    "\n",
    "class NotearsSobolev(nn.Module):\n",
    "    def __init__(self, d, k):\n",
    "        \"\"\"d: num variables k: num expansion of each variable\"\"\"\n",
    "        super(NotearsSobolev, self).__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.fc1_pos = nn.Linear(d * k, d, bias=False)  # ik -> j\n",
    "        self.fc1_neg = nn.Linear(d * k, d, bias=False)\n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "        nn.init.zeros_(self.fc1_pos.weight)\n",
    "        nn.init.zeros_(self.fc1_neg.weight)\n",
    "        self.l2_reg_store = None\n",
    "\n",
    "    def _bounds(self):\n",
    "        # weight shape [j, ik]\n",
    "        bounds = []\n",
    "        for j in range(self.d):\n",
    "            for i in range(self.d):\n",
    "                for _ in range(self.k):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0)\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "\n",
    "    def sobolev_basis(self, x):  # [n, d] -> [n, dk]\n",
    "        seq = []\n",
    "        for kk in range(self.k):\n",
    "            mu = 2.0 / (2 * kk + 1) / math.pi  # sobolev basis\n",
    "            psi = mu * torch.sin(x / mu)\n",
    "            seq.append(psi)  # [n, d] * k\n",
    "        bases = torch.stack(seq, dim=2)  # [n, d, k]\n",
    "        bases = bases.view(-1, self.d * self.k)  # [n, dk]\n",
    "        return bases\n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        bases = self.sobolev_basis(x)  # [n, dk]\n",
    "        x = self.fc1_pos(bases) - self.fc1_neg(bases)  # [n, d]\n",
    "        self.l2_reg_store = torch.sum(x ** 2) / x.shape[0]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
    "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(self.d) + A / self.d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, self.d - 1)\n",
    "        # h = (E.t() * M).sum() - self.d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        reg = self.l2_reg_store\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
    "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fed846-a4e8-4eeb-8137-54a2848032bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## function\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf6c318-b4b1-4c48-a562-c5c7e977e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(output, target):\n",
    "    n = target.shape[0]\n",
    "    loss = 0.5 / n * torch.sum((output - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "def dual_ascent_step(model, X, lambda1, lambda2, rho, alpha, h, rho_max):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian.\"\"\"\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters())\n",
    "    X_torch = torch.from_numpy(X)\n",
    "    while rho < rho_max:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            X_hat = model(X_torch)\n",
    "            loss = squared_loss(X_hat, X_torch)\n",
    "            h_val = model.h_func()\n",
    "            penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            primal_obj = loss + penalty + l2_reg + l1_reg\n",
    "            # primal_obj = loss + penalty\n",
    "            primal_obj.backward()\n",
    "            return primal_obj\n",
    "        optimizer.step(closure)  # NOTE: updates model in-place\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "    alpha += rho * h_new\n",
    "    return rho, alpha, h_new\n",
    "\n",
    "def notears_nonlinear(model: nn.Module,\n",
    "                      X: np.ndarray,\n",
    "                      lambda1: float = 0.,\n",
    "                      lambda2: float = 0.,\n",
    "                      max_iter: int = 100,\n",
    "                      h_tol: float = 1e-8,\n",
    "                      rho_max: float = 1e+16,\n",
    "                      w_threshold: float = 0.3*0.3):\n",
    "    rho, alpha, h = 1.0, 0.0, np.inf\n",
    "    for _ in range(max_iter):\n",
    "        rho, alpha, h = dual_ascent_step(model, X, lambda1, lambda2,\n",
    "                                         rho, alpha, h, rho_max)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = model.fc1_to_adj()\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n",
    "\n",
    "def notears_nonlinear_with_loss_std(model: nn.Module,\n",
    "                                    X_list: list,  \n",
    "                                    lambda1: float = 0.0,\n",
    "                                    lambda2: float = 0.0,\n",
    "                                    max_iter: int = 1000,\n",
    "                                    h_tol: float = 1e-8,\n",
    "                                    rho_max: float = 1e+16,\n",
    "                                    w_threshold: float = 0.3*2.8,\n",
    "                                    std_lambda: float = 1.0,\n",
    "                                    lr: float = 1e-3):  \n",
    "    crp = std_lambda \n",
    "    for _ in range(max_iter):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)  \n",
    "        X_tensors = [torch.from_numpy(X).float().to(torch.double) for X in X_list]\n",
    "        feature_l2_norms_pos_per_predictor = [[] for _ in range(model.dims[0])]\n",
    "        feature_l2_norms_neg_per_predictor = [[] for _ in range(model.dims[0])]\n",
    "        list_all_grads = []\n",
    "        for X in X_tensors:\n",
    "            X_hat = model(X)\n",
    "            loss_mse = squared_loss(X_hat, X)\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            final_loss = (loss_mse + l2_reg + l1_reg) / len(X_tensors)  \n",
    "            optimizer.zero_grad()\n",
    "            final_loss.backward()\n",
    "            grads = []\n",
    "            for param in model.parameters():\n",
    "                grads.append(param.grad.clone().flatten())\n",
    "            all_grads = torch.cat(grads) \n",
    "            list_all_grads.append(all_grads)\n",
    "            for i in range(model.dims[0]):  \n",
    "                pos_g = model.fc1_pos.weight.grad.view(model.dims[0], -1, model.dims[0])[i, :, :]  \n",
    "                neg_g = model.fc1_neg.weight.grad.view(model.dims[0], -1, model.dims[0])[i, :, :]  \n",
    "                l2_norms_pos = torch.norm(pos_g, dim=0, p=2).detach()  \n",
    "                l2_norms_neg = torch.norm(neg_g, dim=0, p=2).detach()  \n",
    "                feature_l2_norms_pos_per_predictor[i].append(l2_norms_pos)\n",
    "                feature_l2_norms_neg_per_predictor[i].append(l2_norms_neg)\n",
    "        list_all_grads = torch.stack(list_all_grads)  \n",
    "        mean_all_grads = torch.mean(list_all_grads, dim=0)\n",
    "        consistency_masks_pos = []\n",
    "        consistency_masks_neg = []\n",
    "        for i in range(model.dims[0]):\n",
    "            ##\n",
    "            feature_l2_norms_pos = torch.stack(feature_l2_norms_pos_per_predictor[i]) \n",
    "            mean_norms_pos = torch.mean(feature_l2_norms_pos, dim=0)  \n",
    "            std_norms_pos = torch.std(feature_l2_norms_pos, dim=0) + 1e-8  \n",
    "            cr_pos = torch.abs(mean_norms_pos) / std_norms_pos  \n",
    "            ct_pos = np.percentile(cr_pos.cpu().numpy(), crp)  \n",
    "            consistency_mask_pos = torch.where(cr_pos >= ct_pos, torch.tensor(1., device=model.fc1_pos.weight.device), torch.tensor(0., device=model.fc1_pos.weight.device))            \n",
    "            consistency_masks_pos.append(consistency_mask_pos.repeat(model.dims[1], 1)) \n",
    "            ##\n",
    "            feature_l2_norms_neg = torch.stack(feature_l2_norms_neg_per_predictor[i]) \n",
    "            mean_norms_neg = torch.mean(feature_l2_norms_neg, dim=0)\n",
    "            std_norms_neg = torch.std(feature_l2_norms_neg, dim=0) + 1e-8\n",
    "            cr_neg = torch.abs(mean_norms_neg) / std_norms_neg  \n",
    "            ct_neg = np.percentile(cr_neg.cpu().numpy(), crp)\n",
    "            consistency_mask_neg = torch.where(cr_neg >= ct_neg, torch.tensor(1., device=model.fc1_neg.weight.device), torch.tensor(0., device=model.fc1_neg.weight.device))\n",
    "            consistency_masks_neg.append(consistency_mask_neg.repeat(model.dims[1], 1))\n",
    "\n",
    "        cmp = torch.stack(consistency_masks_pos).view(-1, model.dims[0])     \n",
    "        cmn = torch.stack(consistency_masks_neg).view(-1, model.dims[0])\n",
    "        start_index = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            param_numel = param.numel()\n",
    "            mean_grad = mean_all_grads[start_index: start_index + param_numel].view_as(param)\n",
    "            if 'fc1_pos.weight' in name:\n",
    "                param.grad = mean_grad * cmp       \n",
    "            elif 'fc1_neg.weight' in name:\n",
    "                param.grad = mean_grad * cmn    \n",
    "            else:\n",
    "                param.grad = mean_grad\n",
    "            start_index += param_numel\n",
    "        optimizer.step()\n",
    "        \n",
    "    W_est = model.fc1_to_adj()\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92c3bdf-b410-456b-bd61-45d007ae18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## initialize\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5a91f6-6052-48ef-9d89-d9bf4fd2a2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchowdh5/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3)\n",
    "ut.set_random_seed(123)\n",
    "n, d, s0, graph_type, sem_type = 200, 10, 40, 'ER', 'mlp'\n",
    "B_true = ut.simulate_dag(d, s0, graph_type)\n",
    "np.savetxt('inputs/W_true.csv', B_true, delimiter=',')\n",
    "# noise_scale = np.ones(d)\n",
    "# X = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale)\n",
    "# np.savetxt('X.csv', X, delimiter=',')\n",
    "noise_scales = [0.2, 1, 2, 5, 10]\n",
    "for i, noise_scale_value in enumerate(noise_scales):\n",
    "    noise_scale = np.full(d, noise_scale_value)  \n",
    "    X = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale)\n",
    "    np.savetxt(f'inputs/X_{i}.csv', X, delimiter=',')  \n",
    "    \n",
    "X_0 = np.loadtxt('inputs/X_0.csv', delimiter=',')\n",
    "X_1 = np.loadtxt('inputs/X_1.csv', delimiter=',')\n",
    "X_2 = np.loadtxt('inputs/X_2.csv', delimiter=',')\n",
    "X_3 = np.loadtxt('inputs/X_3.csv', delimiter=',')\n",
    "X_4 = np.loadtxt('inputs/X_4.csv', delimiter=',')\n",
    "X_list = [X_0, X_1, X_2, X_3, X_4]  # List of datasets\n",
    "scaler = StandardScaler()\n",
    "X_list_standardized = [scaler.fit_transform(X) for X in X_list]\n",
    "X_combined = np.vstack([X for X in X_list_standardized]) \n",
    "print(X_combined.shape)\n",
    "noise_scale_value_test = np.random.uniform(min(noise_scales), max(noise_scales))\n",
    "noise_scale_test = np.full(d, noise_scale_value_test)\n",
    "X_test = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189046e5-1e6a-4c90-885a-8929095a4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## NOTEARS\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db6f0ab1-6427-4b07-b3b8-0d4ff758f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'fdr': 0.36666666666666664, 'tpr': 0.475, 'fpr': 2.2, 'shd': 24, 'nnz': 30}\n",
      "1 {'fdr': 0.36666666666666664, 'tpr': 0.475, 'fpr': 2.2, 'shd': 24, 'nnz': 30}\n",
      "2 {'fdr': 0.36666666666666664, 'tpr': 0.475, 'fpr': 2.2, 'shd': 24, 'nnz': 30}\n",
      "3 {'fdr': 0.36666666666666664, 'tpr': 0.475, 'fpr': 2.2, 'shd': 24, 'nnz': 30}\n",
      "4 {'fdr': 0.36666666666666664, 'tpr': 0.475, 'fpr': 2.2, 'shd': 24, 'nnz': 30}\n",
      "\n",
      "\n",
      "FDR: 0.3667 ± 0.0000\n",
      "SHD: 24.0000 ± 0.0000\n",
      "TPR: 0.4750 ± 0.0000\n",
      "NNZ: 30.0000 ± 0.0000\n",
      "Test squared loss on original values: 726.0235\n"
     ]
    }
   ],
   "source": [
    "list_fdr, list_shd, list_tpr, list_nnz = [], [], [], []\n",
    "for i in range(5):\n",
    "    model = NotearsMLP(dims=[d, 10, 1], bias=True)\n",
    "    W_est = notears_nonlinear(model, X_combined, lambda1=0.01, lambda2=0.01)\n",
    "    # assert ut.is_dag(W_est)\n",
    "    np.savetxt('outputs/W_est.csv', W_est, delimiter=',')\n",
    "    acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "    print(i, acc)\n",
    "    list_fdr.append(acc['fdr'])\n",
    "    list_shd.append(acc['shd'])\n",
    "    list_tpr.append(acc['tpr'])\n",
    "    list_nnz.append(acc['nnz'])    \n",
    "print()\n",
    "print()\n",
    "print(f'FDR: {np.mean(list_fdr):.4f} ± {np.std(list_fdr):.4f}')\n",
    "print(f'SHD: {np.mean(list_shd):.4f} ± {np.std(list_shd):.4f}')\n",
    "print(f'TPR: {np.mean(list_tpr):.4f} ± {np.std(list_tpr):.4f}')\n",
    "print(f'NNZ: {np.mean(list_nnz):.4f} ± {np.std(list_nnz):.4f}')\n",
    "X_hat_test = model(torch.from_numpy(X_test).float().to(torch.double))\n",
    "loss_test = squared_loss(X_hat_test, torch.from_numpy(X_test).float().to(torch.double))\n",
    "print(f'Test squared loss on original values: {loss_test.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e431ab-757c-4e9e-b700-f652e69adcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## invariant NOTEARS\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda79dda-ef7c-4a24-815b-69949f3077f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "std_lambda: 0 and learning_rate: 0.001\n",
      "0 {'fdr': 0.5714285714285714, 'tpr': 0.375, 'fpr': 4.0, 'shd': 34, 'nnz': 35}\n",
      "1 {'fdr': 0.48148148148148145, 'tpr': 0.35, 'fpr': 2.6, 'shd': 33, 'nnz': 27}\n",
      "2 {'fdr': 0.45454545454545453, 'tpr': 0.45, 'fpr': 3.0, 'shd': 33, 'nnz': 33}\n",
      "3 {'fdr': 0.6129032258064516, 'tpr': 0.3, 'fpr': 3.8, 'shd': 36, 'nnz': 31}\n",
      "4 {'fdr': 0.6428571428571429, 'tpr': 0.25, 'fpr': 3.6, 'shd': 39, 'nnz': 28}\n",
      "\n",
      "\n",
      "FDR: 0.5526 ± 0.0732\n",
      "SHD: 35.0000 ± 2.2804\n",
      "TPR: 0.3450 ± 0.0678\n",
      "NNZ: 30.8000 ± 2.9933\n",
      "Test squared loss on original values: 723.5246\n",
      "\n",
      "\n",
      "std_lambda: 0 and learning_rate: 0.01\n",
      "0 {'fdr': 0.43333333333333335, 'tpr': 0.425, 'fpr': 2.6, 'shd': 32, 'nnz': 30}\n",
      "1 {'fdr': 0.6470588235294118, 'tpr': 0.3, 'fpr': 4.4, 'shd': 37, 'nnz': 34}\n",
      "2 {'fdr': 0.5, 'tpr': 0.275, 'fpr': 2.2, 'shd': 33, 'nnz': 22}\n",
      "3 {'fdr': 0.5, 'tpr': 0.4, 'fpr': 3.2, 'shd': 29, 'nnz': 32}\n",
      "4 {'fdr': 0.5, 'tpr': 0.4, 'fpr': 3.2, 'shd': 31, 'nnz': 32}\n",
      "\n",
      "\n",
      "FDR: 0.5161 ± 0.0704\n",
      "SHD: 32.4000 ± 2.6533\n",
      "TPR: 0.3600 ± 0.0604\n",
      "NNZ: 30.0000 ± 4.1952\n",
      "Test squared loss on original values: 721.2901\n",
      "\n",
      "\n",
      "std_lambda: 0 and learning_rate: 0.1\n",
      "0 {'fdr': 0.375, 'tpr': 0.125, 'fpr': 0.6, 'shd': 38, 'nnz': 8}\n",
      "1 {'fdr': 0.6, 'tpr': 0.1, 'fpr': 1.2, 'shd': 40, 'nnz': 10}\n",
      "2 {'fdr': 0.5555555555555556, 'tpr': 0.1, 'fpr': 1.0, 'shd': 39, 'nnz': 9}\n",
      "3 {'fdr': 0.4166666666666667, 'tpr': 0.175, 'fpr': 1.0, 'shd': 38, 'nnz': 12}\n",
      "4 {'fdr': 0.4166666666666667, 'tpr': 0.175, 'fpr': 1.0, 'shd': 38, 'nnz': 12}\n",
      "\n",
      "\n",
      "FDR: 0.4728 ± 0.0882\n",
      "SHD: 38.6000 ± 0.8000\n",
      "TPR: 0.1350 ± 0.0339\n",
      "NNZ: 10.2000 ± 1.6000\n",
      "Test squared loss on original values: 721.9419\n",
      "\n",
      "\n",
      "std_lambda: 10 and learning_rate: 0.001\n",
      "0 {'fdr': 0.5666666666666667, 'tpr': 0.325, 'fpr': 3.4, 'shd': 32, 'nnz': 30}\n",
      "1 {'fdr': 0.5789473684210527, 'tpr': 0.4, 'fpr': 4.4, 'shd': 36, 'nnz': 38}\n",
      "2 {'fdr': 0.5294117647058824, 'tpr': 0.4, 'fpr': 3.6, 'shd': 29, 'nnz': 34}\n",
      "3 {'fdr': 0.47058823529411764, 'tpr': 0.45, 'fpr': 3.2, 'shd': 29, 'nnz': 34}\n",
      "4 {'fdr': 0.46875, 'tpr': 0.425, 'fpr': 3.0, 'shd': 28, 'nnz': 32}\n",
      "\n",
      "\n",
      "FDR: 0.5229 ± 0.0464\n",
      "SHD: 30.8000 ± 2.9257\n",
      "TPR: 0.4000 ± 0.0418\n",
      "NNZ: 33.6000 ± 2.6533\n",
      "Test squared loss on original values: 720.2512\n",
      "\n",
      "\n",
      "std_lambda: 10 and learning_rate: 0.01\n",
      "0 {'fdr': 0.6538461538461539, 'tpr': 0.225, 'fpr': 3.4, 'shd': 39, 'nnz': 26}\n",
      "1 {'fdr': 0.5185185185185185, 'tpr': 0.325, 'fpr': 2.8, 'shd': 35, 'nnz': 27}\n",
      "2 {'fdr': 0.5476190476190477, 'tpr': 0.475, 'fpr': 4.6, 'shd': 33, 'nnz': 42}\n",
      "3 {'fdr': 0.5172413793103449, 'tpr': 0.35, 'fpr': 3.0, 'shd': 32, 'nnz': 29}\n",
      "4 {'fdr': 0.4642857142857143, 'tpr': 0.375, 'fpr': 2.6, 'shd': 30, 'nnz': 28}\n",
      "\n",
      "\n",
      "FDR: 0.5403 ± 0.0628\n",
      "SHD: 33.8000 ± 3.0594\n",
      "TPR: 0.3500 ± 0.0806\n",
      "NNZ: 30.4000 ± 5.8856\n",
      "Test squared loss on original values: 727.4058\n",
      "\n",
      "\n",
      "std_lambda: 10 and learning_rate: 0.1\n",
      "0 {'fdr': 0.5, 'tpr': 0.175, 'fpr': 1.4, 'shd': 38, 'nnz': 14}\n",
      "1 {'fdr': 0.3333333333333333, 'tpr': 0.2, 'fpr': 0.8, 'shd': 36, 'nnz': 12}\n",
      "2 {'fdr': 0.5, 'tpr': 0.15, 'fpr': 1.2, 'shd': 39, 'nnz': 12}\n",
      "3 {'fdr': 0.46153846153846156, 'tpr': 0.175, 'fpr': 1.2, 'shd': 36, 'nnz': 13}\n",
      "4 {'fdr': 0.5, 'tpr': 0.15, 'fpr': 1.2, 'shd': 38, 'nnz': 12}\n",
      "\n",
      "\n",
      "FDR: 0.4590 ± 0.0646\n",
      "SHD: 37.4000 ± 1.2000\n",
      "TPR: 0.1700 ± 0.0187\n",
      "NNZ: 12.6000 ± 0.8000\n",
      "Test squared loss on original values: 722.5915\n",
      "\n",
      "\n",
      "std_lambda: 20 and learning_rate: 0.001\n",
      "0 {'fdr': 0.3333333333333333, 'tpr': 0.55, 'fpr': 2.2, 'shd': 24, 'nnz': 33}\n",
      "1 {'fdr': 0.4722222222222222, 'tpr': 0.475, 'fpr': 3.4, 'shd': 28, 'nnz': 36}\n",
      "2 {'fdr': 0.5555555555555556, 'tpr': 0.4, 'fpr': 4.0, 'shd': 33, 'nnz': 36}\n",
      "3 {'fdr': 0.6666666666666666, 'tpr': 0.275, 'fpr': 4.4, 'shd': 38, 'nnz': 33}\n",
      "4 {'fdr': 0.6486486486486487, 'tpr': 0.325, 'fpr': 4.8, 'shd': 34, 'nnz': 37}\n",
      "\n",
      "\n",
      "FDR: 0.5353 ± 0.1227\n",
      "SHD: 31.4000 ± 4.8826\n",
      "TPR: 0.4050 ± 0.0992\n",
      "NNZ: 35.0000 ± 1.6733\n",
      "Test squared loss on original values: 719.7833\n",
      "\n",
      "\n",
      "std_lambda: 20 and learning_rate: 0.01\n",
      "0 {'fdr': 0.65625, 'tpr': 0.275, 'fpr': 4.2, 'shd': 37, 'nnz': 32}\n",
      "1 {'fdr': 0.5277777777777778, 'tpr': 0.425, 'fpr': 3.8, 'shd': 31, 'nnz': 36}\n",
      "2 {'fdr': 0.5172413793103449, 'tpr': 0.35, 'fpr': 3.0, 'shd': 32, 'nnz': 29}\n",
      "3 {'fdr': 0.6, 'tpr': 0.35, 'fpr': 4.2, 'shd': 37, 'nnz': 35}\n",
      "4 {'fdr': 0.4666666666666667, 'tpr': 0.4, 'fpr': 2.8, 'shd': 33, 'nnz': 30}\n",
      "\n",
      "\n",
      "FDR: 0.5536 ± 0.0667\n",
      "SHD: 34.0000 ± 2.5298\n",
      "TPR: 0.3600 ± 0.0515\n",
      "NNZ: 32.4000 ± 2.7276\n",
      "Test squared loss on original values: 724.3365\n",
      "\n",
      "\n",
      "std_lambda: 20 and learning_rate: 0.1\n",
      "0 {'fdr': 0.7058823529411765, 'tpr': 0.125, 'fpr': 2.4, 'shd': 40, 'nnz': 17}\n",
      "1 {'fdr': 0.46153846153846156, 'tpr': 0.175, 'fpr': 1.2, 'shd': 38, 'nnz': 13}\n",
      "2 {'fdr': 0.4375, 'tpr': 0.225, 'fpr': 1.4, 'shd': 36, 'nnz': 16}\n",
      "3 {'fdr': 0.4666666666666667, 'tpr': 0.2, 'fpr': 1.4, 'shd': 38, 'nnz': 15}\n",
      "4 {'fdr': 0.625, 'tpr': 0.15, 'fpr': 2.0, 'shd': 37, 'nnz': 16}\n",
      "\n",
      "\n",
      "FDR: 0.5393 ± 0.1066\n",
      "SHD: 37.8000 ± 1.3266\n",
      "TPR: 0.1750 ± 0.0354\n",
      "NNZ: 15.4000 ± 1.3565\n",
      "Test squared loss on original values: 720.7887\n",
      "\n",
      "\n",
      "std_lambda: 30 and learning_rate: 0.001\n",
      "0 {'fdr': 0.6046511627906976, 'tpr': 0.425, 'fpr': 5.2, 'shd': 35, 'nnz': 43}\n",
      "1 {'fdr': 0.43243243243243246, 'tpr': 0.525, 'fpr': 3.2, 'shd': 29, 'nnz': 37}\n",
      "2 {'fdr': 0.5675675675675675, 'tpr': 0.4, 'fpr': 4.2, 'shd': 32, 'nnz': 37}\n",
      "3 {'fdr': 0.40625, 'tpr': 0.475, 'fpr': 2.6, 'shd': 27, 'nnz': 32}\n",
      "4 {'fdr': 0.55, 'tpr': 0.45, 'fpr': 4.4, 'shd': 35, 'nnz': 40}\n",
      "\n",
      "\n",
      "FDR: 0.5122 ± 0.0783\n",
      "SHD: 31.6000 ± 3.2000\n",
      "TPR: 0.4550 ± 0.0430\n",
      "NNZ: 37.8000 ± 3.6551\n",
      "Test squared loss on original values: 716.3361\n",
      "\n",
      "\n",
      "std_lambda: 30 and learning_rate: 0.01\n",
      "0 {'fdr': 0.75, 'tpr': 0.15, 'fpr': 3.6, 'shd': 37, 'nnz': 24}\n",
      "1 {'fdr': 0.4666666666666667, 'tpr': 0.4, 'fpr': 2.8, 'shd': 31, 'nnz': 30}\n",
      "2 {'fdr': 0.5454545454545454, 'tpr': 0.375, 'fpr': 3.6, 'shd': 33, 'nnz': 33}\n",
      "3 {'fdr': 0.6571428571428571, 'tpr': 0.3, 'fpr': 4.6, 'shd': 38, 'nnz': 35}\n",
      "4 {'fdr': 0.5, 'tpr': 0.475, 'fpr': 3.8, 'shd': 34, 'nnz': 38}\n",
      "\n",
      "\n",
      "FDR: 0.5839 ± 0.1051\n",
      "SHD: 34.6000 ± 2.5768\n",
      "TPR: 0.3400 ± 0.1102\n",
      "NNZ: 32.0000 ± 4.7749\n",
      "Test squared loss on original values: 725.4497\n",
      "\n",
      "\n",
      "std_lambda: 30 and learning_rate: 0.1\n",
      "0 {'fdr': 0.5714285714285714, 'tpr': 0.225, 'fpr': 2.4, 'shd': 36, 'nnz': 21}\n",
      "1 {'fdr': 0.5555555555555556, 'tpr': 0.2, 'fpr': 2.0, 'shd': 37, 'nnz': 18}\n",
      "2 {'fdr': 0.42857142857142855, 'tpr': 0.3, 'fpr': 1.8, 'shd': 33, 'nnz': 21}\n",
      "3 {'fdr': 0.4117647058823529, 'tpr': 0.25, 'fpr': 1.4, 'shd': 35, 'nnz': 17}\n",
      "4 {'fdr': 0.5, 'tpr': 0.2, 'fpr': 1.6, 'shd': 36, 'nnz': 16}\n",
      "\n",
      "\n",
      "FDR: 0.4935 ± 0.0646\n",
      "SHD: 35.4000 ± 1.3565\n",
      "TPR: 0.2350 ± 0.0374\n",
      "NNZ: 18.6000 ± 2.0591\n",
      "Test squared loss on original values: 717.4870\n",
      "\n",
      "\n",
      "std_lambda: 40 and learning_rate: 0.001\n",
      "0 {'fdr': 0.45714285714285713, 'tpr': 0.475, 'fpr': 3.2, 'shd': 30, 'nnz': 35}\n",
      "1 {'fdr': 0.6875, 'tpr': 0.25, 'fpr': 4.4, 'shd': 37, 'nnz': 32}\n",
      "2 {'fdr': 0.525, 'tpr': 0.475, 'fpr': 4.2, 'shd': 30, 'nnz': 40}\n",
      "3 {'fdr': 0.7058823529411765, 'tpr': 0.25, 'fpr': 4.8, 'shd': 35, 'nnz': 34}\n",
      "4 {'fdr': 0.5135135135135135, 'tpr': 0.45, 'fpr': 3.8, 'shd': 32, 'nnz': 37}\n",
      "\n",
      "\n",
      "FDR: 0.5778 ± 0.0999\n",
      "SHD: 32.8000 ± 2.7857\n",
      "TPR: 0.3800 ± 0.1065\n",
      "NNZ: 35.6000 ± 2.7276\n",
      "Test squared loss on original values: 728.9469\n",
      "\n",
      "\n",
      "std_lambda: 40 and learning_rate: 0.01\n",
      "0 {'fdr': 0.5483870967741935, 'tpr': 0.35, 'fpr': 3.4, 'shd': 33, 'nnz': 31}\n",
      "1 {'fdr': 0.5714285714285714, 'tpr': 0.375, 'fpr': 4.0, 'shd': 32, 'nnz': 35}\n",
      "2 {'fdr': 0.3939393939393939, 'tpr': 0.5, 'fpr': 2.6, 'shd': 27, 'nnz': 33}\n",
      "3 {'fdr': 0.5217391304347826, 'tpr': 0.275, 'fpr': 2.4, 'shd': 38, 'nnz': 23}\n",
      "4 {'fdr': 0.5135135135135135, 'tpr': 0.45, 'fpr': 3.8, 'shd': 33, 'nnz': 37}\n",
      "\n",
      "\n",
      "FDR: 0.5098 ± 0.0614\n",
      "SHD: 32.6000 ± 3.4986\n",
      "TPR: 0.3900 ± 0.0784\n",
      "NNZ: 31.8000 ± 4.8332\n",
      "Test squared loss on original values: 720.6728\n",
      "\n",
      "\n",
      "std_lambda: 40 and learning_rate: 0.1\n",
      "0 {'fdr': 0.5384615384615384, 'tpr': 0.3, 'fpr': 2.8, 'shd': 35, 'nnz': 26}\n",
      "1 {'fdr': 0.391304347826087, 'tpr': 0.35, 'fpr': 1.8, 'shd': 30, 'nnz': 23}\n",
      "2 {'fdr': 0.375, 'tpr': 0.375, 'fpr': 1.8, 'shd': 30, 'nnz': 24}\n",
      "3 {'fdr': 0.5909090909090909, 'tpr': 0.225, 'fpr': 2.6, 'shd': 39, 'nnz': 22}\n",
      "4 {'fdr': 0.47619047619047616, 'tpr': 0.275, 'fpr': 2.0, 'shd': 34, 'nnz': 21}\n",
      "\n",
      "\n",
      "FDR: 0.4744 ± 0.0830\n",
      "SHD: 33.6000 ± 3.3823\n",
      "TPR: 0.3050 ± 0.0534\n",
      "NNZ: 23.2000 ± 1.7205\n",
      "Test squared loss on original values: 722.8949\n",
      "\n",
      "\n",
      "std_lambda: 50 and learning_rate: 0.001\n",
      "0 {'fdr': 0.5909090909090909, 'tpr': 0.45, 'fpr': 5.2, 'shd': 33, 'nnz': 44}\n",
      "1 {'fdr': 0.5675675675675675, 'tpr': 0.4, 'fpr': 4.2, 'shd': 36, 'nnz': 37}\n",
      "2 {'fdr': 0.6896551724137931, 'tpr': 0.225, 'fpr': 4.0, 'shd': 37, 'nnz': 29}\n",
      "3 {'fdr': 0.5277777777777778, 'tpr': 0.425, 'fpr': 3.8, 'shd': 31, 'nnz': 36}\n",
      "4 {'fdr': 0.6052631578947368, 'tpr': 0.375, 'fpr': 4.6, 'shd': 36, 'nnz': 38}\n",
      "\n",
      "\n",
      "FDR: 0.5962 ± 0.0536\n",
      "SHD: 34.6000 ± 2.2450\n",
      "TPR: 0.3750 ± 0.0791\n",
      "NNZ: 36.8000 ± 4.7917\n",
      "Test squared loss on original values: 721.0434\n",
      "\n",
      "\n",
      "std_lambda: 50 and learning_rate: 0.01\n",
      "0 {'fdr': 0.5789473684210527, 'tpr': 0.4, 'fpr': 4.4, 'shd': 30, 'nnz': 38}\n",
      "1 {'fdr': 0.6666666666666666, 'tpr': 0.225, 'fpr': 3.6, 'shd': 38, 'nnz': 27}\n",
      "2 {'fdr': 0.48484848484848486, 'tpr': 0.425, 'fpr': 3.2, 'shd': 32, 'nnz': 33}\n",
      "3 {'fdr': 0.4838709677419355, 'tpr': 0.4, 'fpr': 3.0, 'shd': 29, 'nnz': 31}\n",
      "4 {'fdr': 0.6129032258064516, 'tpr': 0.3, 'fpr': 3.8, 'shd': 38, 'nnz': 31}\n",
      "\n",
      "\n",
      "FDR: 0.5654 ± 0.0719\n",
      "SHD: 33.4000 ± 3.8781\n",
      "TPR: 0.3500 ± 0.0758\n",
      "NNZ: 32.0000 ± 3.5777\n",
      "Test squared loss on original values: 721.7629\n",
      "\n",
      "\n",
      "std_lambda: 50 and learning_rate: 0.1\n",
      "0 {'fdr': 0.5, 'tpr': 0.4, 'fpr': 3.2, 'shd': 35, 'nnz': 32}\n",
      "1 {'fdr': 0.6111111111111112, 'tpr': 0.175, 'fpr': 2.2, 'shd': 36, 'nnz': 18}\n",
      "2 {'fdr': 0.45454545454545453, 'tpr': 0.3, 'fpr': 2.0, 'shd': 33, 'nnz': 22}\n",
      "3 {'fdr': 0.5769230769230769, 'tpr': 0.275, 'fpr': 3.0, 'shd': 37, 'nnz': 26}\n",
      "4 {'fdr': 0.5, 'tpr': 0.225, 'fpr': 1.8, 'shd': 36, 'nnz': 18}\n",
      "\n",
      "\n",
      "FDR: 0.5285 ± 0.0570\n",
      "SHD: 35.4000 ± 1.3565\n",
      "TPR: 0.2750 ± 0.0758\n",
      "NNZ: 23.2000 ± 5.3066\n",
      "Test squared loss on original values: 726.8649\n",
      "\n",
      "\n",
      "std_lambda: 60 and learning_rate: 0.001\n",
      "0 {'fdr': 0.4444444444444444, 'tpr': 0.375, 'fpr': 2.4, 'shd': 32, 'nnz': 27}\n",
      "1 {'fdr': 0.6326530612244898, 'tpr': 0.45, 'fpr': 6.2, 'shd': 35, 'nnz': 49}\n",
      "2 {'fdr': 0.5365853658536586, 'tpr': 0.475, 'fpr': 4.4, 'shd': 34, 'nnz': 41}\n",
      "3 {'fdr': 0.5, 'tpr': 0.475, 'fpr': 3.8, 'shd': 32, 'nnz': 38}\n",
      "4 {'fdr': 0.675, 'tpr': 0.325, 'fpr': 5.4, 'shd': 39, 'nnz': 40}\n",
      "\n",
      "\n",
      "FDR: 0.5577 ± 0.0848\n",
      "SHD: 34.4000 ± 2.5768\n",
      "TPR: 0.4200 ± 0.0600\n",
      "NNZ: 39.0000 ± 7.0711\n",
      "Test squared loss on original values: 721.5292\n",
      "\n",
      "\n",
      "std_lambda: 60 and learning_rate: 0.01\n",
      "0 {'fdr': 0.5294117647058824, 'tpr': 0.4, 'fpr': 3.6, 'shd': 31, 'nnz': 34}\n",
      "1 {'fdr': 0.575, 'tpr': 0.425, 'fpr': 4.6, 'shd': 36, 'nnz': 40}\n",
      "2 {'fdr': 0.4375, 'tpr': 0.45, 'fpr': 2.8, 'shd': 29, 'nnz': 32}\n",
      "3 {'fdr': 0.5294117647058824, 'tpr': 0.4, 'fpr': 3.6, 'shd': 34, 'nnz': 34}\n",
      "4 {'fdr': 0.5555555555555556, 'tpr': 0.4, 'fpr': 4.0, 'shd': 32, 'nnz': 36}\n",
      "\n",
      "\n",
      "FDR: 0.5254 ± 0.0472\n",
      "SHD: 32.4000 ± 2.4166\n",
      "TPR: 0.4150 ± 0.0200\n",
      "NNZ: 35.2000 ± 2.7129\n",
      "Test squared loss on original values: 723.0915\n",
      "\n",
      "\n",
      "std_lambda: 60 and learning_rate: 0.1\n",
      "0 {'fdr': 0.52, 'tpr': 0.3, 'fpr': 2.6, 'shd': 35, 'nnz': 25}\n",
      "1 {'fdr': 0.4642857142857143, 'tpr': 0.375, 'fpr': 2.6, 'shd': 32, 'nnz': 28}\n",
      "2 {'fdr': 0.6, 'tpr': 0.3, 'fpr': 3.6, 'shd': 35, 'nnz': 30}\n",
      "3 {'fdr': 0.7083333333333334, 'tpr': 0.175, 'fpr': 3.4, 'shd': 38, 'nnz': 24}\n",
      "4 {'fdr': 0.5217391304347826, 'tpr': 0.275, 'fpr': 2.4, 'shd': 36, 'nnz': 23}\n",
      "\n",
      "\n",
      "FDR: 0.5629 ± 0.0846\n",
      "SHD: 35.2000 ± 1.9391\n",
      "TPR: 0.2850 ± 0.0644\n",
      "NNZ: 26.0000 ± 2.6077\n",
      "Test squared loss on original values: 726.1057\n",
      "\n",
      "\n",
      "std_lambda: 70 and learning_rate: 0.001\n",
      "0 {'fdr': 0.5135135135135135, 'tpr': 0.45, 'fpr': 3.8, 'shd': 35, 'nnz': 37}\n",
      "1 {'fdr': 0.5116279069767442, 'tpr': 0.525, 'fpr': 4.4, 'shd': 31, 'nnz': 43}\n",
      "2 {'fdr': 0.5151515151515151, 'tpr': 0.4, 'fpr': 3.4, 'shd': 31, 'nnz': 33}\n",
      "3 {'fdr': 0.5882352941176471, 'tpr': 0.35, 'fpr': 4.0, 'shd': 36, 'nnz': 34}\n",
      "4 {'fdr': 0.6666666666666666, 'tpr': 0.35, 'fpr': 5.6, 'shd': 38, 'nnz': 42}\n",
      "\n",
      "\n",
      "FDR: 0.5590 ± 0.0611\n",
      "SHD: 34.2000 ± 2.7857\n",
      "TPR: 0.4150 ± 0.0663\n",
      "NNZ: 37.8000 ± 4.0694\n",
      "Test squared loss on original values: 721.1613\n",
      "\n",
      "\n",
      "std_lambda: 70 and learning_rate: 0.01\n",
      "0 {'fdr': 0.6, 'tpr': 0.4, 'fpr': 4.8, 'shd': 35, 'nnz': 40}\n",
      "1 {'fdr': 0.5789473684210527, 'tpr': 0.4, 'fpr': 4.4, 'shd': 35, 'nnz': 38}\n",
      "2 {'fdr': 0.42857142857142855, 'tpr': 0.4, 'fpr': 2.4, 'shd': 29, 'nnz': 28}\n",
      "3 {'fdr': 0.625, 'tpr': 0.375, 'fpr': 5.0, 'shd': 34, 'nnz': 40}\n",
      "4 {'fdr': 0.65625, 'tpr': 0.275, 'fpr': 4.2, 'shd': 37, 'nnz': 32}\n",
      "\n",
      "\n",
      "FDR: 0.5778 ± 0.0789\n",
      "SHD: 34.0000 ± 2.6833\n",
      "TPR: 0.3700 ± 0.0485\n",
      "NNZ: 35.6000 ± 4.8000\n",
      "Test squared loss on original values: 724.0537\n",
      "\n",
      "\n",
      "std_lambda: 70 and learning_rate: 0.1\n",
      "0 {'fdr': 0.3181818181818182, 'tpr': 0.375, 'fpr': 1.4, 'shd': 28, 'nnz': 22}\n",
      "1 {'fdr': 0.5862068965517241, 'tpr': 0.3, 'fpr': 3.4, 'shd': 35, 'nnz': 29}\n",
      "2 {'fdr': 0.5555555555555556, 'tpr': 0.3, 'fpr': 3.0, 'shd': 34, 'nnz': 27}\n",
      "3 {'fdr': 0.3333333333333333, 'tpr': 0.5, 'fpr': 2.0, 'shd': 28, 'nnz': 30}\n",
      "4 {'fdr': 0.4838709677419355, 'tpr': 0.4, 'fpr': 3.0, 'shd': 32, 'nnz': 31}\n",
      "\n",
      "\n",
      "FDR: 0.4554 ± 0.1111\n",
      "SHD: 31.4000 ± 2.9394\n",
      "TPR: 0.3750 ± 0.0742\n",
      "NNZ: 27.8000 ± 3.1875\n",
      "Test squared loss on original values: 727.3077\n",
      "\n",
      "\n",
      "std_lambda: 80 and learning_rate: 0.001\n",
      "0 {'fdr': 0.5384615384615384, 'tpr': 0.45, 'fpr': 4.2, 'shd': 32, 'nnz': 39}\n",
      "1 {'fdr': 0.5952380952380952, 'tpr': 0.425, 'fpr': 5.0, 'shd': 35, 'nnz': 42}\n",
      "2 {'fdr': 0.5454545454545454, 'tpr': 0.375, 'fpr': 3.6, 'shd': 33, 'nnz': 33}\n",
      "3 {'fdr': 0.43333333333333335, 'tpr': 0.425, 'fpr': 2.6, 'shd': 31, 'nnz': 30}\n",
      "4 {'fdr': 0.5833333333333334, 'tpr': 0.375, 'fpr': 4.2, 'shd': 35, 'nnz': 36}\n",
      "\n",
      "\n",
      "FDR: 0.5392 ± 0.0572\n",
      "SHD: 33.2000 ± 1.6000\n",
      "TPR: 0.4100 ± 0.0300\n",
      "NNZ: 36.0000 ± 4.2426\n",
      "Test squared loss on original values: 720.2174\n",
      "\n",
      "\n",
      "std_lambda: 80 and learning_rate: 0.01\n",
      "0 {'fdr': 0.6470588235294118, 'tpr': 0.3, 'fpr': 4.4, 'shd': 38, 'nnz': 34}\n",
      "1 {'fdr': 0.525, 'tpr': 0.475, 'fpr': 4.2, 'shd': 34, 'nnz': 40}\n",
      "2 {'fdr': 0.5833333333333334, 'tpr': 0.375, 'fpr': 4.2, 'shd': 33, 'nnz': 36}\n",
      "3 {'fdr': 0.5641025641025641, 'tpr': 0.425, 'fpr': 4.4, 'shd': 38, 'nnz': 39}\n",
      "4 {'fdr': 0.6122448979591837, 'tpr': 0.475, 'fpr': 6.0, 'shd': 37, 'nnz': 49}\n",
      "\n",
      "\n",
      "FDR: 0.5863 ± 0.0415\n",
      "SHD: 36.0000 ± 2.0976\n",
      "TPR: 0.4100 ± 0.0663\n",
      "NNZ: 39.6000 ± 5.1614\n",
      "Test squared loss on original values: 723.3955\n",
      "\n",
      "\n",
      "std_lambda: 80 and learning_rate: 0.1\n",
      "0 {'fdr': 0.4411764705882353, 'tpr': 0.475, 'fpr': 3.0, 'shd': 28, 'nnz': 34}\n",
      "1 {'fdr': 0.5882352941176471, 'tpr': 0.35, 'fpr': 4.0, 'shd': 36, 'nnz': 34}\n",
      "2 {'fdr': 0.5384615384615384, 'tpr': 0.3, 'fpr': 2.8, 'shd': 34, 'nnz': 26}\n",
      "3 {'fdr': 0.5357142857142857, 'tpr': 0.325, 'fpr': 3.0, 'shd': 33, 'nnz': 28}\n",
      "4 {'fdr': 0.5517241379310345, 'tpr': 0.325, 'fpr': 3.2, 'shd': 36, 'nnz': 29}\n",
      "\n",
      "\n",
      "FDR: 0.5311 ± 0.0487\n",
      "SHD: 33.4000 ± 2.9394\n",
      "TPR: 0.3550 ± 0.0620\n",
      "NNZ: 30.2000 ± 3.2496\n",
      "Test squared loss on original values: 728.2123\n",
      "\n",
      "\n",
      "std_lambda: 90 and learning_rate: 0.001\n",
      "0 {'fdr': 0.5128205128205128, 'tpr': 0.475, 'fpr': 4.0, 'shd': 32, 'nnz': 39}\n",
      "1 {'fdr': 0.5641025641025641, 'tpr': 0.425, 'fpr': 4.4, 'shd': 36, 'nnz': 39}\n",
      "2 {'fdr': 0.5945945945945946, 'tpr': 0.375, 'fpr': 4.4, 'shd': 32, 'nnz': 37}\n",
      "3 {'fdr': 0.6052631578947368, 'tpr': 0.375, 'fpr': 4.6, 'shd': 36, 'nnz': 38}\n",
      "4 {'fdr': 0.5, 'tpr': 0.475, 'fpr': 3.8, 'shd': 31, 'nnz': 38}\n",
      "\n",
      "\n",
      "FDR: 0.5554 ± 0.0424\n",
      "SHD: 33.4000 ± 2.1541\n",
      "TPR: 0.4250 ± 0.0447\n",
      "NNZ: 38.2000 ± 0.7483\n",
      "Test squared loss on original values: 718.1834\n",
      "\n",
      "\n",
      "std_lambda: 90 and learning_rate: 0.01\n",
      "0 {'fdr': 0.6410256410256411, 'tpr': 0.35, 'fpr': 5.0, 'shd': 37, 'nnz': 39}\n",
      "1 {'fdr': 0.5853658536585366, 'tpr': 0.425, 'fpr': 4.8, 'shd': 32, 'nnz': 41}\n",
      "2 {'fdr': 0.6, 'tpr': 0.35, 'fpr': 4.2, 'shd': 30, 'nnz': 35}\n",
      "3 {'fdr': 0.5625, 'tpr': 0.35, 'fpr': 3.6, 'shd': 35, 'nnz': 32}\n",
      "4 {'fdr': 0.5526315789473685, 'tpr': 0.425, 'fpr': 4.2, 'shd': 32, 'nnz': 38}\n",
      "\n",
      "\n",
      "FDR: 0.5883 ± 0.0312\n",
      "SHD: 33.2000 ± 2.4819\n",
      "TPR: 0.3800 ± 0.0367\n",
      "NNZ: 37.0000 ± 3.1623\n",
      "Test squared loss on original values: 720.4901\n",
      "\n",
      "\n",
      "std_lambda: 90 and learning_rate: 0.1\n",
      "0 {'fdr': 0.525, 'tpr': 0.475, 'fpr': 4.2, 'shd': 31, 'nnz': 40}\n",
      "1 {'fdr': 0.5263157894736842, 'tpr': 0.45, 'fpr': 4.0, 'shd': 38, 'nnz': 38}\n",
      "2 {'fdr': 0.5428571428571428, 'tpr': 0.4, 'fpr': 3.8, 'shd': 33, 'nnz': 35}\n",
      "3 {'fdr': 0.5238095238095238, 'tpr': 0.5, 'fpr': 4.4, 'shd': 32, 'nnz': 42}\n",
      "4 {'fdr': 0.43333333333333335, 'tpr': 0.425, 'fpr': 2.6, 'shd': 28, 'nnz': 30}\n",
      "\n",
      "\n",
      "FDR: 0.5103 ± 0.0391\n",
      "SHD: 32.4000 ± 3.2619\n",
      "TPR: 0.4500 ± 0.0354\n",
      "NNZ: 37.0000 ± 4.1952\n",
      "Test squared loss on original values: 722.0617\n",
      "\n",
      "\n",
      "std_lambda: 100 and learning_rate: 0.001\n",
      "0 {'fdr': 0.5789473684210527, 'tpr': 0.4, 'fpr': 4.4, 'shd': 31, 'nnz': 38}\n",
      "1 {'fdr': 0.4878048780487805, 'tpr': 0.525, 'fpr': 4.0, 'shd': 30, 'nnz': 41}\n",
      "2 {'fdr': 0.5151515151515151, 'tpr': 0.4, 'fpr': 3.4, 'shd': 31, 'nnz': 33}\n",
      "3 {'fdr': 0.5945945945945946, 'tpr': 0.375, 'fpr': 4.4, 'shd': 37, 'nnz': 37}\n",
      "4 {'fdr': 0.5757575757575758, 'tpr': 0.35, 'fpr': 3.8, 'shd': 36, 'nnz': 33}\n",
      "\n",
      "\n",
      "FDR: 0.5505 ± 0.0414\n",
      "SHD: 33.0000 ± 2.8983\n",
      "TPR: 0.4100 ± 0.0604\n",
      "NNZ: 36.4000 ± 3.0725\n",
      "Test squared loss on original values: 721.0508\n",
      "\n",
      "\n",
      "std_lambda: 100 and learning_rate: 0.01\n",
      "0 {'fdr': 0.5, 'tpr': 0.35, 'fpr': 2.8, 'shd': 30, 'nnz': 28}\n",
      "1 {'fdr': 0.46511627906976744, 'tpr': 0.575, 'fpr': 4.0, 'shd': 32, 'nnz': 43}\n",
      "2 {'fdr': 0.6774193548387096, 'tpr': 0.25, 'fpr': 4.2, 'shd': 39, 'nnz': 31}\n",
      "3 {'fdr': 0.425, 'tpr': 0.575, 'fpr': 3.4, 'shd': 27, 'nnz': 40}\n",
      "4 {'fdr': 0.5405405405405406, 'tpr': 0.425, 'fpr': 4.0, 'shd': 32, 'nnz': 37}\n",
      "\n",
      "\n",
      "FDR: 0.5216 ± 0.0867\n",
      "SHD: 32.0000 ± 3.9497\n",
      "TPR: 0.4350 ± 0.1271\n",
      "NNZ: 35.8000 ± 5.5642\n",
      "Test squared loss on original values: 720.8099\n",
      "\n",
      "\n",
      "std_lambda: 100 and learning_rate: 0.1\n",
      "0 {'fdr': 0.5675675675675675, 'tpr': 0.4, 'fpr': 4.2, 'shd': 35, 'nnz': 37}\n",
      "1 {'fdr': 0.5111111111111111, 'tpr': 0.55, 'fpr': 4.6, 'shd': 31, 'nnz': 45}\n",
      "2 {'fdr': 0.3448275862068966, 'tpr': 0.475, 'fpr': 2.0, 'shd': 26, 'nnz': 29}\n",
      "3 {'fdr': 0.65, 'tpr': 0.35, 'fpr': 5.2, 'shd': 33, 'nnz': 40}\n",
      "4 {'fdr': 0.5348837209302325, 'tpr': 0.5, 'fpr': 4.6, 'shd': 33, 'nnz': 43}\n",
      "\n",
      "\n",
      "FDR: 0.5217 ± 0.1001\n",
      "SHD: 31.6000 ± 3.0725\n",
      "TPR: 0.4550 ± 0.0714\n",
      "NNZ: 38.8000 ± 5.6000\n",
      "Test squared loss on original values: 719.1767\n"
     ]
    }
   ],
   "source": [
    "lsl = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "llr = [1e-3, 1e-2, 1e-1]\n",
    "\n",
    "for sl in lsl:\n",
    "    for lr in llr:\n",
    "        print()\n",
    "        print()\n",
    "        print(f'std_lambda: {sl} and learning_rate: {lr}')\n",
    "        list_fdr, list_shd, list_tpr, list_nnz = [], [], [], []\n",
    "        for i in range(5):\n",
    "            model = NotearsMLP(dims=[d, 10, 1], bias=True)\n",
    "            W_est = notears_nonlinear_with_loss_std(model, X_list_standardized, lambda1=0.01, lambda2=0.01, std_lambda=sl, lr=lr)\n",
    "            # assert ut.is_dag(W_est)\n",
    "            np.savetxt('outputs/W_est2.csv', W_est, delimiter=',')\n",
    "            acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "            print(i, acc)\n",
    "            list_fdr.append(acc['fdr'])\n",
    "            list_shd.append(acc['shd'])\n",
    "            list_tpr.append(acc['tpr'])\n",
    "            list_nnz.append(acc['nnz'])        \n",
    "        print()\n",
    "        print()\n",
    "        print(f'FDR: {np.mean(list_fdr):.4f} ± {np.std(list_fdr):.4f}')\n",
    "        print(f'SHD: {np.mean(list_shd):.4f} ± {np.std(list_shd):.4f}')\n",
    "        print(f'TPR: {np.mean(list_tpr):.4f} ± {np.std(list_tpr):.4f}')\n",
    "        print(f'NNZ: {np.mean(list_nnz):.4f} ± {np.std(list_nnz):.4f}')\n",
    "        X_hat_test = model(torch.from_numpy(X_test).float().to(torch.double))\n",
    "        loss_test = squared_loss(X_hat_test, torch.from_numpy(X_test).float().to(torch.double))\n",
    "        print(f'Test squared loss on original values: {loss_test.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd8756b6-339f-4d88-8bf2-6c245779c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a=2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c87ed4-2357-43f3-8f22-ae0ca97ffd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d7d1c-3603-477a-b21b-8bb20bde92a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
