{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b905c959-b42e-4bbf-8f7f-205a43d8157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load and save with proper encoding for Excel compatibility\n",
    "# for file_name in [\"result2.csv\", \"result3.csv\"]:\n",
    "#     # Load the CSV file\n",
    "#     df = pd.read_csv(file_name)\n",
    "    \n",
    "#     # Save with UTF-8 encoding that Excel recognizes properly\n",
    "#     df.to_csv(file_name, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(\"Files saved with proper encoding: result2.csv, result3.csv\")\n",
    "\n",
    "# file_1 = \"result2.csv\"\n",
    "# file_2 = \"result3.csv\"\n",
    "\n",
    "# # Read both CSVs\n",
    "# df1 = pd.read_csv(file_1)\n",
    "# df2 = pd.read_csv(file_2)\n",
    "\n",
    "# # Merge them by concatenating rows\n",
    "# merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# merged_df.head()\n",
    "\n",
    "# merged_df.columns\n",
    "\n",
    "# # Desired column order\n",
    "# column_order = ['ntrials', 'n', 'd', 's0', 'gt', 'st', 'nh', 'l1', 'l2', 'wt', 'lr', 'ct', 'fdr', 'shd', 'tpr', 'nnz']\n",
    "\n",
    "# # Reorder columns in the merged DataFrame\n",
    "# merged_df = merged_df[column_order]\n",
    "\n",
    "# merged_df.head()\n",
    "\n",
    "# # Sort the DataFrame based on the specified columns\n",
    "# sort_columns = ['ntrials', 'n', 'd', 's0', 'gt', 'st', 'nh', 'l1', 'l2', 'wt', 'lr', 'ct']\n",
    "# sorted_df = merged_df.sort_values(by=sort_columns, ignore_index=True)\n",
    "\n",
    "# sorted_df.head(20)\n",
    "\n",
    "# # Save the sorted DataFrame with the name result2.csv\n",
    "# sorted_df.to_csv(\"result2.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(\"Sorted DataFrame saved as result2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef067218-65cb-4a03-b905-5cf1cfeb776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## notears: input to output with fc1 and fc2\n",
    "## [n, d] = [n, 10] \n",
    "##        = [n x 10] x [10 x 100] # this is fc1_pos also fc1_neg they have dimension [d, d*dims[1]] which is [10, 100]\n",
    "##        = [n x 100]\n",
    "##        = [n x 10 x 10] x [10 x 10 x 1] # this is fc2, has dimension [10, 10, 1], so for each feature it has separate [10, 1] weights\n",
    "##        = [n x 10 x 1]\n",
    "##        = [n x 10]\n",
    "##        = [n, 10] = [n, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54540896-5dfd-4d8d-8330-ccd1aa2e4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## notears + cglearn: how to separate 'd' predictors, so we have to separate the weights for each of 'd' targets\n",
    "##\n",
    "## for i in range(model.dims[0]):  # here we separate weight for 'i'-th target\n",
    "##     pos_g = model.fc1_pos.weight.grad.view(model.dims[0], -1, model.dims[0])[i, :, :] # [target, m1, input]\n",
    "##     neg_g = model.fc1_neg.weight.grad.view(model.dims[0], -1, model.dims[0])[i, :, :] # [target, m1, input]\n",
    "##\n",
    "## so now pos_g has weight shaped [m1, input] for each target, so now we have separated the target, so now focusing on only one target that is the i-th\n",
    "## \n",
    "##     l2_norms_pos = torch.norm(pos_g, dim=0, p=2).detach() \n",
    "##     l2_norms_neg = torch.norm(neg_g, dim=0, p=2).detach() \n",
    "##\n",
    "## taking L2-norm over 'm1', so now for each target we have the L2-norm for each input\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf2cb67-5711-41c0-b043-e315275da4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## comment\n",
    "###################################\n",
    "# few things to check -\n",
    "#         1. data generation => n, d, s0, graph_type, sem_type = 200, 10, 40, 'ER', 'mlp'\n",
    "#         2. env generation =>  noise_scales = [0.2, 1, 2, 5, 10]\n",
    "#         3. nhidden=10, lambda1=0.01, lambda2=0.01, w_threshold=0.3, std_lambda=60, learning_rate=0.001\n",
    "#         4. in utils.count_accuracy() => is_dag() is disabled/enabled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bec7f38-6c14-4605-a80a-81664d53b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## install and import\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f206b63c-40ff-4f55-94ac-c3d9330196be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchowdh5/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-igraph\n",
    "from notears.locally_connected import LocallyConnected\n",
    "from notears.lbfgsb_scipy import LBFGSBScipy\n",
    "from notears.trace_expm import trace_expm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import notears.utils as ut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b87020-6b18-4d0a-996b-230a6a2b9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## class\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37d2c7e-e15d-4802-b5a1-71bec5d9435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotearsMLP(nn.Module):\n",
    "    def __init__(self, dims, bias=True):\n",
    "        super(NotearsMLP, self).__init__()\n",
    "        assert len(dims) >= 2\n",
    "        assert dims[-1] == 1\n",
    "        d = dims[0]\n",
    "        self.dims = dims\n",
    "\n",
    "        # fc1: variable splitting for l1\n",
    "        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "\n",
    "        # fc2: Locally connected layers with BatchNorm\n",
    "        layers = []\n",
    "        for l in range(len(dims) - 2):\n",
    "            layers.append(LocallyConnected(d, dims[l + 1], dims[l + 2], bias=bias))\n",
    "        self.fc2 = nn.ModuleList(layers)\n",
    "\n",
    "    def _bounds(self):\n",
    "        d = self.dims[0]\n",
    "        bounds = []\n",
    "        for j in range(d):\n",
    "            for m in range(self.dims[1]):\n",
    "                for i in range(d):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0)\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        # Apply fc1 and normalize\n",
    "        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1]\n",
    "        x = x.view(-1, self.dims[0], self.dims[1])  # Reshape to [n, d, m1]\n",
    "\n",
    "        # Apply fc2 layers and normalization\n",
    "        for fc in self.fc2:\n",
    "            x = torch.sigmoid(x)  # Activation\n",
    "            x = fc(x)  # Locally connected layer output [n, d, m2]\n",
    "\n",
    "        x = x.squeeze(dim=2)  # [n, d]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        \"\"\"Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(d) + A / d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, d - 1)\n",
    "        # h = (E.t() * M).sum() - d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        \"\"\"Take 2-norm-squared of all parameters\"\"\"\n",
    "        reg = 0.\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        reg += torch.sum(fc1_weight ** 2)\n",
    "        for fc in self.fc2:\n",
    "            reg += torch.sum(fc.weight ** 2)\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        \"\"\"Take l1 norm of fc1 weight\"\"\"\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n",
    "        \"\"\"Get W from fc1 weights, take 2-norm over m1 dim\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W\n",
    "\n",
    "class NotearsMLP2(nn.Module):\n",
    "    def __init__(self, dims, bias=True):\n",
    "        super(NotearsMLP2, self).__init__()\n",
    "        assert len(dims) >= 2\n",
    "        assert dims[-1] == 1\n",
    "        d = dims[0]\n",
    "        self.dims = dims\n",
    "\n",
    "        # fc1: variable splitting for l1\n",
    "        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "\n",
    "        # Layer normalization after fc1\n",
    "        self.ln1 = nn.LayerNorm(d * dims[1])  # Normalize across feature dimensions\n",
    "\n",
    "        # fc2: Locally connected layers with BatchNorm\n",
    "        layers = []\n",
    "        for l in range(len(dims) - 2):\n",
    "            layers.append(LocallyConnected(d, dims[l + 1], dims[l + 2], bias=bias))\n",
    "        self.fc2 = nn.ModuleList(layers)\n",
    "\n",
    "    def _bounds(self):\n",
    "        d = self.dims[0]\n",
    "        bounds = []\n",
    "        for j in range(d):\n",
    "            for m in range(self.dims[1]):\n",
    "                for i in range(d):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0)\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "    \n",
    "    def _apply_bounds(self):\n",
    "        \"\"\"Clip weights of fc1_pos and fc1_neg to stay within bounds.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for layer, bounds in [(self.fc1_pos, self.fc1_pos.weight.bounds), (self.fc1_neg, self.fc1_neg.weight.bounds)]:\n",
    "                # Apply bounds per weight\n",
    "                for idx, (min_bound, max_bound) in enumerate(bounds):\n",
    "                    if max_bound is not None:  # Clip only if max_bound is defined\n",
    "                        layer.weight.data.view(-1)[idx].clamp_(min_bound, max_bound)    \n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        # Apply fc1 and normalize\n",
    "        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1]\n",
    "        self._apply_bounds()  # Enforce bounds on fc1 weights\n",
    "        x = self.ln1(x)  # Apply LayerNorm after fc1\n",
    "        x = x.view(-1, self.dims[0], self.dims[1])  # Reshape to [n, d, m1]\n",
    "\n",
    "        # Apply fc2 layers and normalization\n",
    "        for fc in self.fc2:\n",
    "            x = torch.sigmoid(x)  # Activation\n",
    "            x = fc(x)  # Locally connected layer output [n, d, m2]\n",
    "\n",
    "        x = x.squeeze(dim=2)  # [n, d]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        \"\"\"Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(d) + A / d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, d - 1)\n",
    "        # h = (E.t() * M).sum() - d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        \"\"\"Take 2-norm-squared of all parameters\"\"\"\n",
    "        reg = 0.\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        reg += torch.sum(fc1_weight ** 2)\n",
    "        for fc in self.fc2:\n",
    "            reg += torch.sum(fc.weight ** 2)\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        \"\"\"Take l1 norm of fc1 weight\"\"\"\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n",
    "        \"\"\"Get W from fc1 weights, take 2-norm over m1 dim\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W\n",
    "\n",
    "class NotearsSobolev(nn.Module):\n",
    "    def __init__(self, d, k):\n",
    "        \"\"\"d: num variables k: num expansion of each variable\"\"\"\n",
    "        super(NotearsSobolev, self).__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.fc1_pos = nn.Linear(d * k, d, bias=False)  # ik -> j\n",
    "        self.fc1_neg = nn.Linear(d * k, d, bias=False)\n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "        nn.init.zeros_(self.fc1_pos.weight)\n",
    "        nn.init.zeros_(self.fc1_neg.weight)\n",
    "        self.l2_reg_store = None\n",
    "\n",
    "    def _bounds(self):\n",
    "        # weight shape [j, ik]\n",
    "        bounds = []\n",
    "        for j in range(self.d):\n",
    "            for i in range(self.d):\n",
    "                for _ in range(self.k):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0)\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "\n",
    "    def sobolev_basis(self, x):  # [n, d] -> [n, dk]\n",
    "        seq = []\n",
    "        for kk in range(self.k):\n",
    "            mu = 2.0 / (2 * kk + 1) / math.pi  # sobolev basis\n",
    "            psi = mu * torch.sin(x / mu)\n",
    "            seq.append(psi)  # [n, d] * k\n",
    "        bases = torch.stack(seq, dim=2)  # [n, d, k]\n",
    "        bases = bases.view(-1, self.d * self.k)  # [n, dk]\n",
    "        return bases\n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        bases = self.sobolev_basis(x)  # [n, dk]\n",
    "        x = self.fc1_pos(bases) - self.fc1_neg(bases)  # [n, d]\n",
    "        self.l2_reg_store = torch.sum(x ** 2) / x.shape[0]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
    "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(self.d) + A / self.d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, self.d - 1)\n",
    "        # h = (E.t() * M).sum() - self.d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        reg = self.l2_reg_store\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
    "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd9ad90-b333-4e9a-8573-84b9b297509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## function\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7ba48e-90c0-4cec-a83b-d6de397d4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(output, target):\n",
    "    n = target.shape[0]\n",
    "    loss = 0.5 / n * torch.sum((output - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "def notears_nonlinear(model: nn.Module,\n",
    "                      X: np.ndarray,\n",
    "                      lambda1: float = 0.,\n",
    "                      lambda2: float = 0.,\n",
    "                      max_iter: int = 100,\n",
    "                      h_tol: float = 1e-8,\n",
    "                      rho_max: float = 1e+16,\n",
    "                      w_threshold: float = 0.3):\n",
    "    model.w_threshold = w_threshold\n",
    "    rho, alpha, h = 1.0, 0.0, np.inf\n",
    "    for _ in range(max_iter):\n",
    "        rho, alpha, h = dual_ascent_step(model, X, lambda1, lambda2,\n",
    "                                         rho, alpha, h, rho_max)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = model.fc1_to_adj()\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n",
    "\n",
    "def notears_nonlinear_with_loss_std(model: nn.Module,\n",
    "                                    X_list: list,  \n",
    "                                    lambda1: float = 0.0,\n",
    "                                    lambda2: float = 0.0,\n",
    "                                    max_iter: int = 100,\n",
    "                                    h_tol: float = 1e-8,\n",
    "                                    rho_max: float = 1e+16,\n",
    "                                    w_threshold: float = 0.3,\n",
    "                                    std_lambda: float = 1.0,\n",
    "                                    lr: float = 1e-3\n",
    "                                   ):  \n",
    "    model.w_threshold = w_threshold    \n",
    "    rho, alpha, h = 1.0, 0.0, np.inf\n",
    "    for iter_no in range(max_iter):\n",
    "        rho, alpha, h = dual_ascent_step_with_loss_std(\n",
    "            model, X_list, lambda1, lambda2, std_lambda, rho, alpha, h, rho_max, iter_no, lr=lr)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = model.fc1_to_adj()\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n",
    "\n",
    "def dual_ascent_step(model, X, lambda1, lambda2, rho, alpha, h, rho_max):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian.\"\"\"\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters())\n",
    "    X_torch = torch.from_numpy(X)\n",
    "    while rho < rho_max:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            X_hat = model(X_torch)\n",
    "            loss = squared_loss(X_hat, X_torch)\n",
    "            h_val = model.h_func()\n",
    "            penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            primal_obj = loss + penalty + l2_reg + l1_reg\n",
    "            # primal_obj = loss + penalty\n",
    "            primal_obj.backward()\n",
    "            return primal_obj\n",
    "        optimizer.step(closure)  # NOTE: updates model in-place\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "    alpha += rho * h_new\n",
    "    return rho, alpha, h_new\n",
    "\n",
    "def bak_dual_ascent_step_with_loss_std(model, X_list, lambda1, lambda2, std_lambda, rho, alpha, h, rho_max, iter_no, lr=0.001):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian, with consistent gradient-based learning (CGLearn) for each predictor.\"\"\"\n",
    "    \n",
    "    crp = std_lambda  # Consistency ratio percentile\n",
    "    h_new = None\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Using Adam optimizer instead of LBFGSBScipy\n",
    "    X_tensors = [torch.from_numpy(X).float().to(torch.double).to(model.fc1_pos.weight.device) for X in X_list]  # Ensure tensors are on the correct device\n",
    "    \n",
    "    while rho < rho_max:\n",
    "        feature_l2_norms_pos_per_predictor = [[] for _ in range(model.dims[0])]  # Separate L2 norms for fc1_pos\n",
    "        feature_l2_norms_neg_per_predictor = [[] for _ in range(model.dims[0])]  # Separate L2 norms for fc1_neg\n",
    "        list_all_grads = []  # To store gradients for the entire model (all layers)\n",
    "\n",
    "        # Step 1: Compute loss and gradients for each environment\n",
    "        for X in X_tensors:\n",
    "            # Loss calculation\n",
    "            X_hat = model(X)\n",
    "            loss_mse = squared_loss(X_hat, X)\n",
    "            h_val = model.h_func()\n",
    "            penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            final_loss = (loss_mse + penalty + l2_reg + l1_reg) / len(X_tensors)  # Averaged across environments\n",
    "\n",
    "            # Grad calculation\n",
    "            optimizer.zero_grad()\n",
    "            final_loss.backward()\n",
    "\n",
    "            # Collect all gradients for the entire model (for all layers)\n",
    "            grads = []\n",
    "            for param in model.parameters():\n",
    "                grads.append(param.grad.clone().flatten())\n",
    "            all_grads = torch.cat(grads)  # Flatten and concatenate all gradients for all parameters\n",
    "            list_all_grads.append(all_grads)  # Store gradients for this dataset\n",
    "\n",
    "            # Compute and store gradients per predictor for the first hidden layer\n",
    "            for i in range(model.dims[0]):  # Iterate over each predictor (input feature)\n",
    "                # Get the gradients for the first hidden layer (fc1_pos and fc1_neg)\n",
    "                pos_g = model.fc1_pos.weight.grad.view(model.dims[0], -1, model.dims[0])[i, :, :]  # Shape [5, 10]\n",
    "                neg_g = model.fc1_neg.weight.grad.view(model.dims[0], -1, model.dims[0])[i, :, :]  # Shape [5, 10]\n",
    "                \n",
    "                ## \n",
    "                \n",
    "                # Compute L2 norms of the gradients across the hidden neurons for each feature\n",
    "                l2_norms_pos = torch.norm(pos_g, dim=0, p=2).detach()  # L2 norm across hidden neurons, result shape: [10]\n",
    "                l2_norms_neg = torch.norm(neg_g, dim=0, p=2).detach()  # Same for fc1_neg\n",
    "\n",
    "                # Store L2 norms for each predictor\n",
    "                feature_l2_norms_pos_per_predictor[i].append(l2_norms_pos)\n",
    "                feature_l2_norms_neg_per_predictor[i].append(l2_norms_neg)\n",
    "\n",
    "        # Step 2: Compute mean of all gradients for the entire model across all datasets\n",
    "        list_all_grads = torch.stack(list_all_grads)  # Stack all gradients for all datasets\n",
    "        mean_all_grads = torch.mean(list_all_grads, dim=0)  # Mean gradient for the entire model\n",
    "\n",
    "        # Step 3: Gradient consistency calculations (CGLearn) per predictor\n",
    "        consistency_masks_pos = []\n",
    "        consistency_masks_neg = []\n",
    "        \n",
    "        for i in range(model.dims[0]):  # Iterate over each predictor (input feature)\n",
    "            # Consistency check for fc1_pos weights of predictor i\n",
    "            feature_l2_norms_pos = torch.stack(feature_l2_norms_pos_per_predictor[i])  # Stack L2 norms across datasets\n",
    "            mean_norms_pos = torch.mean(feature_l2_norms_pos, dim=0)  # Mean of L2 norms for each feature\n",
    "            std_norms_pos = torch.std(feature_l2_norms_pos, dim=0) + 1e-8  # Standard deviation of L2 norms for each feature\n",
    "            cr_pos = torch.abs(mean_norms_pos) / std_norms_pos  # Consistency ratio\n",
    "            ct_pos = np.percentile(cr_pos.cpu().numpy(), crp)  # Threshold based on the percentile\n",
    "            consistency_mask_pos = torch.where(cr_pos >= ct_pos, torch.tensor(1., device=model.fc1_pos.weight.device), torch.tensor(0., device=model.fc1_pos.weight.device))            \n",
    "            consistency_masks_pos.append(consistency_mask_pos.repeat(model.dims[1], 1))  # Broadcast the mask\n",
    "            \n",
    "            # Consistency check for fc1_neg weights of predictor i\n",
    "            feature_l2_norms_neg = torch.stack(feature_l2_norms_neg_per_predictor[i])  # Same for fc1_neg\n",
    "            mean_norms_neg = torch.mean(feature_l2_norms_neg, dim=0)\n",
    "            std_norms_neg = torch.std(feature_l2_norms_neg, dim=0) + 1e-8\n",
    "            cr_neg = torch.abs(mean_norms_neg) / std_norms_neg  # Consistency ratio\n",
    "            ct_neg = np.percentile(cr_neg.cpu().numpy(), crp)\n",
    "            consistency_mask_neg = torch.where(cr_neg >= ct_neg, torch.tensor(1., device=model.fc1_neg.weight.device), torch.tensor(0., device=model.fc1_neg.weight.device))\n",
    "            consistency_masks_neg.append(consistency_mask_neg.repeat(model.dims[1], 1))\n",
    "            \n",
    "        # Step 4: Apply the masks to gradients before updating parameters\n",
    "        cmp = torch.stack(consistency_masks_pos).view(-1, model.dims[0])     \n",
    "        cmn = torch.stack(consistency_masks_neg).view(-1, model.dims[0])\n",
    "        \n",
    "        start_index = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            param_numel = param.numel()\n",
    "            mean_grad = mean_all_grads[start_index: start_index + param_numel].view_as(param)\n",
    "            if 'fc1_pos.weight' in name:\n",
    "                param.grad = mean_grad * cmp       \n",
    "            elif 'fc1_neg.weight' in name:\n",
    "                param.grad = mean_grad * cmn    \n",
    "            else:\n",
    "                param.grad = mean_grad\n",
    "            start_index += param_numel\n",
    "\n",
    "        # Step 5: Now that the gradients have been modified, perform the optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step 6: Check convergence and adjust rho\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # ww = model.fc1_to_adj()\n",
    "        # ww[np.abs(ww) < model.w_threshold] = 0\n",
    "        # print(ww)\n",
    "\n",
    "    alpha += rho * h_new\n",
    "    return rho, alpha, h_new\n",
    "\n",
    "def dual_ascent_step_with_loss_std(model, X_list, lambda1, lambda2, std_lambda, rho, alpha, h, rho_max, iter_no, lr=0.001):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian, with consistent gradient-based learning (CGLearn) for each predictor.\"\"\"\n",
    "    \n",
    "    crp = std_lambda  # Consistency ratio percentile\n",
    "    h_new = None\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Using Adam optimizer instead of LBFGSBScipy\n",
    "    X_tensors = [torch.from_numpy(X).float().to(torch.double).to(model.fc1_pos.weight.device) for X in X_list]  # Ensure tensors are on the correct device\n",
    "    \n",
    "    while rho < rho_max:\n",
    "        list_all_grads = []  # To store gradients for the entire model (all layers)\n",
    "\n",
    "        # Step 1: Compute loss and gradients for each environment\n",
    "        for X in X_tensors:\n",
    "            # Loss calculation\n",
    "            X_hat = model(X)\n",
    "            loss_mse = squared_loss(X_hat, X)\n",
    "            h_val = model.h_func()\n",
    "            penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            final_loss = (loss_mse + penalty + l2_reg + l1_reg) / len(X_tensors)  # Averaged across environments\n",
    "            # Grad calculation\n",
    "            optimizer.zero_grad()\n",
    "            final_loss.backward()\n",
    "            # Collect all gradients for the entire model (for all layers)\n",
    "            grads = []\n",
    "            for param in model.parameters():\n",
    "                grads.append(param.grad.clone().flatten())\n",
    "            all_grads = torch.cat(grads)  # Flatten and concatenate all gradients for all parameters\n",
    "            list_all_grads.append(all_grads)  # Store gradients for this dataset\n",
    "            \n",
    "        # Step 2: Compute consistency mask\n",
    "        list_all_grads = torch.stack(list_all_grads)  # Stack all gradients for all datasets\n",
    "        mean_all_grads = torch.mean(list_all_grads, dim=0)  # Mean gradient for the entire model\n",
    "        std_all_grads = torch.std(list_all_grads, dim=0) + 1e-8  # StdDev gradient for the entire model\n",
    "        cr_all_grads = torch.abs(mean_all_grads) / std_all_grads \n",
    "        ct_all_grads = np.percentile(cr_all_grads.cpu().numpy(), crp) \n",
    "        cm_all_grads = torch.where(cr_all_grads >= ct_all_grads, torch.tensor(1., device=model.fc1_pos.weight.device), torch.tensor(0., device=model.fc1_pos.weight.device))            \n",
    "\n",
    "        # step 3: update the mean gradient based on consistency\n",
    "        updated_mean_all_grads = mean_all_grads * cm_all_grads \n",
    "\n",
    "        # step 4: update the parameters with updated mean gradient\n",
    "        start_index = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            param_numel = param.numel()\n",
    "            mean_grad = updated_mean_all_grads[start_index: start_index + param_numel].view_as(param)\n",
    "            if 'fc1_pos.weight' in name:\n",
    "                param.grad = mean_grad       \n",
    "            elif 'fc1_neg.weight' in name:\n",
    "                param.grad = mean_grad    \n",
    "            else:\n",
    "                param.grad = mean_grad\n",
    "            start_index += param_numel\n",
    "\n",
    "        # Step 5: Now that the gradients have been modified, perform the optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step 6: Check convergence and adjust rho\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # ww = model.fc1_to_adj()\n",
    "        # ww[np.abs(ww) < model.w_threshold] = 0\n",
    "        # print(ww)\n",
    "\n",
    "    alpha += rho * h_new\n",
    "    return rho, alpha, h_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f208b1eb-9c75-42ed-aa6b-b8351ae9b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a772fe4-b8a8-40bc-8bc0-6c088e204cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001)\n",
      "1\n",
      "FDR: 0.5670 ± 0.0442\n",
      "SHD: 10.4000 ± 0.8000\n",
      "TPR: 0.3200 ± 0.1470\n",
      "NNZ: 7.4000 ± 3.1369\n",
      "\n",
      "2\n",
      "FDR: 0.8884 ± 0.0010\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 89.6000 ± 0.8000\n",
      "\n",
      "3\n",
      "FDR: 0.8884 ± 0.0010\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 89.6000 ± 0.8000\n",
      "\n",
      "4\n",
      "FDR: 0.8889 ± 0.0000\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 90.0000 ± 0.0000\n",
      "\n",
      "5\n",
      "FDR: 0.8886 ± 0.0005\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 89.8000 ± 0.4000\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/scipy/linalg/_matfuncs.py:378: RuntimeWarning: overflow encountered in matmul\n",
      "  eAw = eAw @ eAw\n",
      "/opt/anaconda3/lib/python3.10/site-packages/scipy/linalg/_matfuncs.py:378: RuntimeWarning: invalid value encountered in matmul\n",
      "  eAw = eAw @ eAw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01)\n",
      "1\n",
      "FDR: 0.5593 ± 0.0513\n",
      "SHD: 10.2000 ± 0.7483\n",
      "TPR: 0.3200 ± 0.1470\n",
      "NNZ: 7.2000 ± 2.7857\n",
      "\n",
      "2\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 10.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "3\n",
      "FDR: 0.2000 ± 0.4000\n",
      "SHD: 10.2000 ± 0.4000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.2000 ± 0.4000\n",
      "\n",
      "4\n",
      "FDR: 0.5000 ± 0.4472\n",
      "SHD: 9.8000 ± 0.9798\n",
      "TPR: 0.0600 ± 0.0800\n",
      "NNZ: 1.4000 ± 0.8000\n",
      "\n",
      "5\n",
      "FDR: 0.8654 ± 0.0104\n",
      "SHD: 43.2000 ± 1.1662\n",
      "TPR: 0.9400 ± 0.0490\n",
      "NNZ: 70.0000 ± 1.8974\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05)\n",
      "1\n",
      "FDR: 0.5760 ± 0.0420\n",
      "SHD: 10.4000 ± 0.8000\n",
      "TPR: 0.3000 ± 0.1095\n",
      "NNZ: 7.2000 ± 2.7857\n",
      "\n",
      "2\n",
      "FDR: 1.0000 ± 0.0000\n",
      "SHD: 14.8000 ± 1.6000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 4.8000 ± 1.6000\n",
      "\n",
      "3\n",
      "FDR: 0.2000 ± 0.4000\n",
      "SHD: 10.8000 ± 1.6000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.8000 ± 1.6000\n",
      "\n",
      "4\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 9.2000 ± 1.1662\n",
      "TPR: 0.0800 ± 0.1166\n",
      "NNZ: 0.8000 ± 1.1662\n",
      "\n",
      "5\n",
      "FDR: 0.4448 ± 0.3099\n",
      "SHD: 9.8000 ± 1.7205\n",
      "TPR: 0.1600 ± 0.1356\n",
      "NNZ: 3.8000 ± 2.3152\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1)\n",
      "1\n",
      "FDR: 0.5957 ± 0.0566\n",
      "SHD: 10.8000 ± 0.9798\n",
      "TPR: 0.3000 ± 0.1095\n",
      "NNZ: 7.6000 ± 3.0067\n",
      "\n",
      "2\n",
      "FDR: 0.9432 ± 0.0383\n",
      "SHD: 41.8000 ± 10.9800\n",
      "TPR: 0.3000 ± 0.2000\n",
      "NNZ: 44.4000 ± 17.2116\n",
      "\n",
      "3\n",
      "FDR: 0.2000 ± 0.4000\n",
      "SHD: 10.6000 ± 1.2000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.6000 ± 1.2000\n",
      "\n",
      "4\n",
      "FDR: 0.8378 ± 0.2157\n",
      "SHD: 12.4000 ± 3.5553\n",
      "TPR: 0.2000 ± 0.2757\n",
      "NNZ: 8.2000 ± 4.3081\n",
      "\n",
      "5\n",
      "FDR: 0.7776 ± 0.0508\n",
      "SHD: 20.6000 ± 4.2708\n",
      "TPR: 0.5400 ± 0.0490\n",
      "NNZ: 25.4000 ± 5.0833\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001)\n",
      "1\n",
      "FDR: 0.4984 ± 0.1182\n",
      "SHD: 33.4000 ± 2.2450\n",
      "TPR: 0.2000 ± 0.0570\n",
      "NNZ: 15.8000 ± 1.7205\n",
      "\n",
      "2\n",
      "FDR: 0.5556 ± 0.0000\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 90.0000 ± 0.0000\n",
      "\n",
      "3\n",
      "FDR: 0.5568 ± 0.0025\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 0.9950 ± 0.0100\n",
      "NNZ: 89.8000 ± 0.4000\n",
      "\n",
      "4\n",
      "FDR: 0.5556 ± 0.0000\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 90.0000 ± 0.0000\n",
      "\n",
      "5\n",
      "FDR: 0.5556 ± 0.0000\n",
      "SHD: 45.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 90.0000 ± 0.0000\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/scipy/linalg/_matfuncs.py:378: RuntimeWarning: overflow encountered in matmul\n",
      "  eAw = eAw @ eAw\n",
      "/opt/anaconda3/lib/python3.10/site-packages/scipy/linalg/_matfuncs.py:378: RuntimeWarning: invalid value encountered in matmul\n",
      "  eAw = eAw @ eAw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01)\n",
      "1\n",
      "FDR: 0.5188 ± 0.1301\n",
      "SHD: 34.2000 ± 1.9391\n",
      "TPR: 0.1850 ± 0.0515\n",
      "NNZ: 15.6000 ± 2.7276\n",
      "\n",
      "2\n",
      "FDR: 0.2000 ± 0.4000\n",
      "SHD: 40.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.2000 ± 0.4000\n",
      "\n",
      "3\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 40.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "4\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 40.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "5\n",
      "FDR: 0.5353 ± 0.0663\n",
      "SHD: 36.4000 ± 4.9639\n",
      "TPR: 0.8300 ± 0.1155\n",
      "NNZ: 71.6000 ± 4.3635\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05)\n",
      "1\n",
      "FDR: 0.5250 ± 0.1130\n",
      "SHD: 34.2000 ± 1.9391\n",
      "TPR: 0.1850 ± 0.0515\n",
      "NNZ: 15.6000 ± 2.2450\n",
      "\n",
      "2\n",
      "FDR: 1.0000 ± 0.0000\n",
      "SHD: 41.6000 ± 0.8000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 1.6000 ± 0.8000\n",
      "\n",
      "3\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 40.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "4\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 40.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "5\n",
      "FDR: 0.4133 ± 0.3250\n",
      "SHD: 38.4000 ± 0.8000\n",
      "TPR: 0.0450 ± 0.0245\n",
      "NNZ: 2.8000 ± 1.3266\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1)\n",
      "1\n",
      "FDR: 0.5188 ± 0.1301\n",
      "SHD: 34.2000 ± 1.9391\n",
      "TPR: 0.1850 ± 0.0515\n",
      "NNZ: 15.6000 ± 2.7276\n",
      "\n",
      "2\n",
      "FDR: 0.6970 ± 0.1595\n",
      "SHD: 43.0000 ± 8.0747\n",
      "TPR: 0.4850 ± 0.3212\n",
      "NNZ: 52.6000 ± 28.8347\n",
      "\n",
      "3\n",
      "FDR: 0.2103 ± 0.2581\n",
      "SHD: 40.6000 ± 1.7436\n",
      "TPR: 0.2000 ± 0.3876\n",
      "NNZ: 17.8000 ± 34.6087\n",
      "\n",
      "4\n",
      "FDR: 0.6400 ± 0.2518\n",
      "SHD: 38.4000 ± 2.0591\n",
      "TPR: 0.0600 ± 0.0515\n",
      "NNZ: 5.6000 ± 2.9394\n",
      "\n",
      "5\n",
      "FDR: 0.5216 ± 0.1186\n",
      "SHD: 32.0000 ± 3.7417\n",
      "TPR: 0.2600 ± 0.0903\n",
      "NNZ: 21.6000 ± 4.2237\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001)\n",
      "1\n",
      "FDR: 0.5384 ± 0.1482\n",
      "SHD: 19.4000 ± 2.8705\n",
      "TPR: 0.3000 ± 0.0632\n",
      "NNZ: 13.8000 ± 3.7094\n",
      "\n",
      "2\n",
      "FDR: 0.9486 ± 0.0018\n",
      "SHD: 189.6000 ± 0.8000\n",
      "TPR: 0.9500 ± 0.0316\n",
      "NNZ: 369.8000 ± 2.4819\n",
      "\n",
      "3\n",
      "FDR: 0.9478 ± 0.0014\n",
      "SHD: 189.8000 ± 0.4000\n",
      "TPR: 0.9700 ± 0.0245\n",
      "NNZ: 371.4000 ± 2.9394\n",
      "\n",
      "4\n",
      "FDR: 0.9477 ± 0.0022\n",
      "SHD: 189.8000 ± 0.4000\n",
      "TPR: 0.9800 ± 0.0400\n",
      "NNZ: 375.0000 ± 1.5492\n",
      "\n",
      "5\n",
      "FDR: 0.9467 ± 0.0004\n",
      "SHD: 190.0000 ± 0.0000\n",
      "TPR: 1.0000 ± 0.0000\n",
      "NNZ: 375.4000 ± 2.7276\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01)\n",
      "1\n",
      "FDR: 0.5460 ± 0.1545\n",
      "SHD: 19.4000 ± 2.8705\n",
      "TPR: 0.2900 ± 0.0490\n",
      "NNZ: 13.8000 ± 4.0200\n",
      "\n",
      "2\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 20.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "3\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 20.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "4\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 19.8000 ± 0.4000\n",
      "TPR: 0.0100 ± 0.0200\n",
      "NNZ: 0.2000 ± 0.4000\n",
      "\n",
      "5\n",
      "FDR: 0.8803 ± 0.0093\n",
      "SHD: 106.2000 ± 6.7350\n",
      "TPR: 0.7500 ± 0.0548\n",
      "NNZ: 125.6000 ± 8.1388\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05)\n",
      "1\n",
      "FDR: 0.5526 ± 0.1547\n",
      "SHD: 19.6000 ± 2.7276\n",
      "TPR: 0.2900 ± 0.0490\n",
      "NNZ: 14.0000 ± 4.0000\n",
      "\n",
      "2\n",
      "FDR: 0.2000 ± 0.4000\n",
      "SHD: 20.2000 ± 0.4000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.2000 ± 0.4000\n",
      "\n",
      "3\n",
      "FDR: 0.1000 ± 0.2000\n",
      "SHD: 19.8000 ± 0.4000\n",
      "TPR: 0.0100 ± 0.0200\n",
      "NNZ: 0.4000 ± 0.8000\n",
      "\n",
      "4\n",
      "FDR: 0.1600 ± 0.3200\n",
      "SHD: 19.2000 ± 2.1354\n",
      "TPR: 0.0600 ± 0.0970\n",
      "NNZ: 2.0000 ± 2.4495\n",
      "\n",
      "5\n",
      "FDR: 0.8860 ± 0.0267\n",
      "SHD: 57.0000 ± 5.1769\n",
      "TPR: 0.3200 ± 0.0748\n",
      "NNZ: 56.2000 ± 3.9699\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1)\n",
      "1\n",
      "FDR: 0.5526 ± 0.1547\n",
      "SHD: 19.6000 ± 2.7276\n",
      "TPR: 0.2900 ± 0.0490\n",
      "NNZ: 14.0000 ± 4.0000\n",
      "\n",
      "2\n",
      "FDR: 0.7695 ± 0.3853\n",
      "SHD: 121.0000 ± 82.0878\n",
      "TPR: 0.5700 ± 0.4686\n",
      "NNZ: 224.2000 ± 182.7965\n",
      "\n",
      "3\n",
      "FDR: 0.7648 ± 0.0998\n",
      "SHD: 53.8000 ± 51.7664\n",
      "TPR: 0.4500 ± 0.1225\n",
      "NNZ: 66.6000 ± 75.3382\n",
      "\n",
      "4\n",
      "FDR: 0.8462 ± 0.0998\n",
      "SHD: 30.2000 ± 8.2801\n",
      "TPR: 0.2700 ± 0.3010\n",
      "NNZ: 24.0000 ± 22.6186\n",
      "\n",
      "5\n",
      "FDR: 0.8765 ± 0.0220\n",
      "SHD: 91.2000 ± 6.4622\n",
      "TPR: 0.6700 ± 0.1122\n",
      "NNZ: 109.2000 ± 10.2450\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001)\n",
      "1\n",
      "FDR: 0.5635 ± 0.0773\n",
      "SHD: 76.0000 ± 4.6904\n",
      "TPR: 0.1800 ± 0.0491\n",
      "NNZ: 32.6000 ± 5.6071\n",
      "\n",
      "2\n",
      "FDR: 0.7898 ± 0.0040\n",
      "SHD: 187.6000 ± 1.3565\n",
      "TPR: 0.9675 ± 0.0203\n",
      "NNZ: 368.2000 ± 2.1354\n",
      "\n",
      "3\n",
      "FDR: 0.7875 ± 0.0020\n",
      "SHD: 188.6000 ± 1.0198\n",
      "TPR: 0.9875 ± 0.0137\n",
      "NNZ: 371.8000 ± 3.7630\n",
      "\n",
      "4\n",
      "FDR: 0.7903 ± 0.0013\n",
      "SHD: 189.0000 ± 0.6325\n",
      "TPR: 0.9800 ± 0.0061\n",
      "NNZ: 373.8000 ± 0.9798\n",
      "\n",
      "5\n",
      "FDR: 0.7886 ± 0.0014\n",
      "SHD: 189.6000 ± 0.8000\n",
      "TPR: 0.9950 ± 0.0061\n",
      "NNZ: 376.6000 ± 1.8547\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01)\n",
      "1\n",
      "FDR: 0.5788 ± 0.0912\n",
      "SHD: 76.2000 ± 4.4900\n",
      "TPR: 0.1750 ± 0.0576\n",
      "NNZ: 32.4000 ± 6.2801\n",
      "\n",
      "2\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 80.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "3\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 80.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "4\n",
      "FDR: 0.0000 ± 0.0000\n",
      "SHD: 80.0000 ± 0.0000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.0000 ± 0.0000\n",
      "\n",
      "5\n",
      "FDR: 0.7878 ± 0.0327\n",
      "SHD: 117.6000 ± 7.7614\n",
      "TPR: 0.3050 ± 0.0562\n",
      "NNZ: 114.8000 ± 7.8588\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05)\n",
      "1\n",
      "FDR: 0.5788 ± 0.0912\n",
      "SHD: 76.2000 ± 4.4900\n",
      "TPR: 0.1750 ± 0.0576\n",
      "NNZ: 32.4000 ± 6.2801\n",
      "\n",
      "2\n",
      "FDR: 0.2000 ± 0.4000\n",
      "SHD: 80.2000 ± 0.4000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.2000 ± 0.4000\n",
      "\n",
      "3\n",
      "FDR: 0.2000 ± 0.4000\n",
      "SHD: 80.2000 ± 0.4000\n",
      "TPR: 0.0000 ± 0.0000\n",
      "NNZ: 0.2000 ± 0.4000\n",
      "\n",
      "4\n",
      "FDR: 0.4000 ± 0.3742\n",
      "SHD: 80.0000 ± 0.6325\n",
      "TPR: 0.0200 ± 0.0232\n",
      "NNZ: 3.2000 ± 3.6551\n",
      "\n",
      "5\n",
      "FDR: 0.7380 ± 0.0515\n",
      "SHD: 85.4000 ± 1.7436\n",
      "TPR: 0.1050 ± 0.0595\n",
      "NNZ: 29.8000 ± 13.4075\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n",
      "\n",
      "\n",
      "####################################################################\n",
      "(200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1)\n",
      "1\n",
      "FDR: 0.5756 ± 0.0952\n",
      "SHD: 76.0000 ± 4.6904\n",
      "TPR: 0.1750 ± 0.0576\n",
      "NNZ: 32.2000 ± 6.2418\n",
      "\n",
      "2\n",
      "FDR: 0.7560 ± 0.0703\n",
      "SHD: 168.4000 ± 42.2071\n",
      "TPR: 0.8100 ± 0.3433\n",
      "NNZ: 305.8000 ± 139.9920\n",
      "\n",
      "3\n",
      "FDR: 0.7478 ± 0.0615\n",
      "SHD: 122.6000 ± 48.5823\n",
      "TPR: 0.4000 ± 0.3736\n",
      "NNZ: 149.4000 ± 149.5789\n",
      "\n",
      "4\n",
      "FDR: 0.6645 ± 0.1243\n",
      "SHD: 83.8000 ± 5.3442\n",
      "TPR: 0.0950 ± 0.0341\n",
      "NNZ: 25.4000 ± 9.8102\n",
      "\n",
      "5\n",
      "FDR: 0.7319 ± 0.0936\n",
      "SHD: 104.0000 ± 16.7451\n",
      "TPR: 0.2875 ± 0.0925\n",
      "NNZ: 91.6000 ± 24.1131\n",
      "\n",
      "####################################################################\n",
      "\n",
      "\n",
      "CSV saved as result2.csv\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3)\n",
    "list_res = []\n",
    "column_titles = [\"ntrials\", \"sample\", \"node\", \"edge\", \"graph\", \"SEM\", \"hiddenU\", \"l1\", \"l2\", \"wthresh\", \"lr\", \"cthresh\", \"fdr\", \"shd\", \"tpr\", \"nnz\"]\n",
    "df = pd.DataFrame(list_res, columns=column_titles)\n",
    "df.to_csv(\"result2.csv\", index=False, encoding=\"utf-8-sig\")  \n",
    "print(\"CSV saved as result2.csv\")\n",
    "##############################################################\n",
    "########## set parameters for experiments\n",
    "ntrials = 5\n",
    "list_opt = [ ## + check number of trials\n",
    "    (200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001),\n",
    "    (200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01),\n",
    "    (200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05),\n",
    "    (200, 10, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1),\n",
    "    (200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001),\n",
    "    (200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01),\n",
    "    (200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05),\n",
    "    (200, 10, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1),\n",
    "    (200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001),\n",
    "    (200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01),\n",
    "    (200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05),\n",
    "    (200, 20, 1, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1),\n",
    "    (200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.001),\n",
    "    (200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.01),\n",
    "    (200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.05),\n",
    "    (200, 20, 4, 'ER', 'mlp', 10, 0.01, 0.01, 0.3, 0.1),    \n",
    "]\n",
    "##############################################################\n",
    "for opt in list_opt:\n",
    "    n, d, _s0, graph_type, sem_type, nh, l1, l2, wt, lr = opt\n",
    "    s0 = d*_s0\n",
    "    list_fdr, list_shd, list_tpr, list_nnz = [], [], [], []\n",
    "    list_fdr2, list_shd2, list_tpr2, list_nnz2 = [], [], [], []    \n",
    "    list_fdr3, list_shd3, list_tpr3, list_nnz3 = [], [], [], []    \n",
    "    list_fdr4, list_shd4, list_tpr4, list_nnz4 = [], [], [], []    \n",
    "    list_fdr5, list_shd5, list_tpr5, list_nnz5 = [], [], [], []        \n",
    "    for tn in range(ntrials):\n",
    "        ##\n",
    "        ## initialize\n",
    "        ##\n",
    "        ut.set_random_seed(123+tn)\n",
    "        B_true = ut.simulate_dag(d, s0, graph_type)\n",
    "        np.savetxt('inputs/W_true.csv', B_true, delimiter=',')\n",
    "        # noise_scale = np.ones(d)\n",
    "        # X = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale)\n",
    "        # np.savetxt('X.csv', X, delimiter=',')\n",
    "        noise_scales = [0.2, 1, 2, 5, 10]\n",
    "        for i, noise_scale_value in enumerate(noise_scales):\n",
    "            noise_scale = np.full(d, noise_scale_value)  \n",
    "            X = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale)\n",
    "            np.savetxt(f'inputs/X_{i}.csv', X, delimiter=',')  \n",
    "\n",
    "        X_0 = np.loadtxt('inputs/X_0.csv', delimiter=',')\n",
    "        X_1 = np.loadtxt('inputs/X_1.csv', delimiter=',')\n",
    "        X_2 = np.loadtxt('inputs/X_2.csv', delimiter=',')\n",
    "        X_3 = np.loadtxt('inputs/X_3.csv', delimiter=',')\n",
    "        X_4 = np.loadtxt('inputs/X_4.csv', delimiter=',')\n",
    "        X_list = [X_0, X_1, X_2, X_3, X_4]  # List of datasets\n",
    "        scaler = StandardScaler()\n",
    "        X_list_standardized = [scaler.fit_transform(X) for X in X_list] ## separate fit_transform cause each dataset with different scale (diff noise scale)\n",
    "        X_combined = np.vstack([X for X in X_list_standardized]) \n",
    "        ##\n",
    "        ## notears\n",
    "        ##\n",
    "        model = NotearsMLP(dims=[d, nh, 1], bias=True)\n",
    "        W_est = notears_nonlinear(model, X_combined, lambda1=l1, lambda2=l2, w_threshold=wt)\n",
    "        # assert ut.is_dag(W_est)\n",
    "        np.savetxt('outputs/W_est.csv', W_est, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "        list_fdr.append(acc['fdr'])\n",
    "        list_shd.append(acc['shd'])\n",
    "        list_tpr.append(acc['tpr'])\n",
    "        list_nnz.append(acc['nnz'])   \n",
    "        ##\n",
    "        ## inotears\n",
    "        ## 2\n",
    "        model = NotearsMLP2(dims=[d, nh, 1], bias=True)\n",
    "        W_est = notears_nonlinear_with_loss_std(model, X_list_standardized, lambda1=l1, lambda2=l2, std_lambda=20, w_threshold=wt, lr=lr)\n",
    "        # assert ut.is_dag(W_est)\n",
    "        np.savetxt('outputs/W_est2.csv', W_est, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "        list_fdr2.append(acc['fdr'])\n",
    "        list_shd2.append(acc['shd'])\n",
    "        list_tpr2.append(acc['tpr'])\n",
    "        list_nnz2.append(acc['nnz'])        \n",
    "        ## 3\n",
    "        model = NotearsMLP2(dims=[d, nh, 1], bias=True)\n",
    "        W_est = notears_nonlinear_with_loss_std(model, X_list_standardized, lambda1=l1, lambda2=l2, std_lambda=40, w_threshold=wt, lr=lr)\n",
    "        # assert ut.is_dag(W_est)\n",
    "        np.savetxt('outputs/W_est3.csv', W_est, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "        list_fdr3.append(acc['fdr'])\n",
    "        list_shd3.append(acc['shd'])\n",
    "        list_tpr3.append(acc['tpr'])\n",
    "        list_nnz3.append(acc['nnz'])        \n",
    "        ## 4\n",
    "        model = NotearsMLP2(dims=[d, nh, 1], bias=True)\n",
    "        W_est = notears_nonlinear_with_loss_std(model, X_list_standardized, lambda1=l1, lambda2=l2, std_lambda=60, w_threshold=wt, lr=lr)\n",
    "        # assert ut.is_dag(W_est)\n",
    "        np.savetxt('outputs/W_est4.csv', W_est, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "        list_fdr4.append(acc['fdr'])\n",
    "        list_shd4.append(acc['shd'])\n",
    "        list_tpr4.append(acc['tpr'])\n",
    "        list_nnz4.append(acc['nnz'])        \n",
    "        ## 5\n",
    "        model = NotearsMLP2(dims=[d, nh, 1], bias=True)\n",
    "        W_est = notears_nonlinear_with_loss_std(model, X_list_standardized, lambda1=l1, lambda2=l2, std_lambda=80, w_threshold=wt, lr=lr)\n",
    "        # assert ut.is_dag(W_est)\n",
    "        np.savetxt('outputs/W_est5.csv', W_est, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "        list_fdr5.append(acc['fdr'])\n",
    "        list_shd5.append(acc['shd'])\n",
    "        list_tpr5.append(acc['tpr'])\n",
    "        list_nnz5.append(acc['nnz'])        \n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print('####################################################################')\n",
    "    print(opt)\n",
    "    print(1)\n",
    "    print(f'FDR: {np.mean(list_fdr):.4f} ± {np.std(list_fdr):.4f}')\n",
    "    print(f'SHD: {np.mean(list_shd):.4f} ± {np.std(list_shd):.4f}')\n",
    "    print(f'TPR: {np.mean(list_tpr):.4f} ± {np.std(list_tpr):.4f}')\n",
    "    print(f'NNZ: {np.mean(list_nnz):.4f} ± {np.std(list_nnz):.4f}')\n",
    "    res = (\n",
    "        str(ntrials), str(n), str(d), str(s0), str(graph_type), str(sem_type), str(nh), str(l1), str(l2), str(wt), str('NA'), \n",
    "        str('reg Notears'), \n",
    "        f'{np.mean(list_fdr):.4f} ± {np.std(list_fdr):.4f}', \n",
    "        f'{np.mean(list_shd):.4f} ± {np.std(list_shd):.4f}',\n",
    "        f'{np.mean(list_tpr):.4f} ± {np.std(list_tpr):.4f}',\n",
    "        f'{np.mean(list_nnz):.4f} ± {np.std(list_nnz):.4f}'\n",
    "    )\n",
    "    list_res.append(res)\n",
    "    print()\n",
    "    \n",
    "    print(2)\n",
    "    print(f'FDR: {np.mean(list_fdr2):.4f} ± {np.std(list_fdr2):.4f}')\n",
    "    print(f'SHD: {np.mean(list_shd2):.4f} ± {np.std(list_shd2):.4f}')\n",
    "    print(f'TPR: {np.mean(list_tpr2):.4f} ± {np.std(list_tpr2):.4f}')\n",
    "    print(f'NNZ: {np.mean(list_nnz2):.4f} ± {np.std(list_nnz2):.4f}')\n",
    "    res = (\n",
    "        str(ntrials), str(n), str(d), str(s0), str(graph_type), str(sem_type), str(nh), str(l1), str(l2), str(wt), str(lr), \n",
    "        str(20), \n",
    "        f'{np.mean(list_fdr2):.4f} ± {np.std(list_fdr2):.4f}', \n",
    "        f'{np.mean(list_shd2):.4f} ± {np.std(list_shd2):.4f}',\n",
    "        f'{np.mean(list_tpr2):.4f} ± {np.std(list_tpr2):.4f}',\n",
    "        f'{np.mean(list_nnz2):.4f} ± {np.std(list_nnz2):.4f}'\n",
    "    )\n",
    "    list_res.append(res)    \n",
    "    print()\n",
    "    \n",
    "    print(3)\n",
    "    print(f'FDR: {np.mean(list_fdr3):.4f} ± {np.std(list_fdr3):.4f}')\n",
    "    print(f'SHD: {np.mean(list_shd3):.4f} ± {np.std(list_shd3):.4f}')\n",
    "    print(f'TPR: {np.mean(list_tpr3):.4f} ± {np.std(list_tpr3):.4f}')\n",
    "    print(f'NNZ: {np.mean(list_nnz3):.4f} ± {np.std(list_nnz3):.4f}')\n",
    "    res = (\n",
    "        str(ntrials), str(n), str(d), str(s0), str(graph_type), str(sem_type), str(nh), str(l1), str(l2), str(wt), str(lr), \n",
    "        str(40), \n",
    "        f'{np.mean(list_fdr3):.4f} ± {np.std(list_fdr3):.4f}', \n",
    "        f'{np.mean(list_shd3):.4f} ± {np.std(list_shd3):.4f}',\n",
    "        f'{np.mean(list_tpr3):.4f} ± {np.std(list_tpr3):.4f}',\n",
    "        f'{np.mean(list_nnz3):.4f} ± {np.std(list_nnz3):.4f}'\n",
    "    )\n",
    "    list_res.append(res)    \n",
    "    print()\n",
    "    \n",
    "    print(4)\n",
    "    print(f'FDR: {np.mean(list_fdr4):.4f} ± {np.std(list_fdr4):.4f}')\n",
    "    print(f'SHD: {np.mean(list_shd4):.4f} ± {np.std(list_shd4):.4f}')\n",
    "    print(f'TPR: {np.mean(list_tpr4):.4f} ± {np.std(list_tpr4):.4f}')\n",
    "    print(f'NNZ: {np.mean(list_nnz4):.4f} ± {np.std(list_nnz4):.4f}')\n",
    "    res = (\n",
    "        str(ntrials), str(n), str(d), str(s0), str(graph_type), str(sem_type), str(nh), str(l1), str(l2), str(wt), str(lr), \n",
    "        str(60), \n",
    "        f'{np.mean(list_fdr4):.4f} ± {np.std(list_fdr4):.4f}', \n",
    "        f'{np.mean(list_shd4):.4f} ± {np.std(list_shd4):.4f}',\n",
    "        f'{np.mean(list_tpr4):.4f} ± {np.std(list_tpr4):.4f}',\n",
    "        f'{np.mean(list_nnz4):.4f} ± {np.std(list_nnz4):.4f}'\n",
    "    )\n",
    "    list_res.append(res)    \n",
    "    print()\n",
    "    \n",
    "    print(5)\n",
    "    print(f'FDR: {np.mean(list_fdr5):.4f} ± {np.std(list_fdr5):.4f}')\n",
    "    print(f'SHD: {np.mean(list_shd5):.4f} ± {np.std(list_shd5):.4f}')\n",
    "    print(f'TPR: {np.mean(list_tpr5):.4f} ± {np.std(list_tpr5):.4f}')\n",
    "    print(f'NNZ: {np.mean(list_nnz5):.4f} ± {np.std(list_nnz5):.4f}')\n",
    "    res = (\n",
    "        str(ntrials), str(n), str(d), str(s0), str(graph_type), str(sem_type), str(nh), str(l1), str(l2), str(wt), str(lr), \n",
    "        str(80), \n",
    "        f'{np.mean(list_fdr5):.4f} ± {np.std(list_fdr5):.4f}', \n",
    "        f'{np.mean(list_shd5):.4f} ± {np.std(list_shd5):.4f}',\n",
    "        f'{np.mean(list_tpr5):.4f} ± {np.std(list_tpr5):.4f}',\n",
    "        f'{np.mean(list_nnz5):.4f} ± {np.std(list_nnz5):.4f}'\n",
    "    )\n",
    "    list_res.append(res)    \n",
    "    print()\n",
    "    print('####################################################################')\n",
    "    print()\n",
    "    print()\n",
    "    df = pd.DataFrame(list_res, columns=column_titles)\n",
    "    df.to_csv(\"result2.csv\", index=False, encoding=\"utf-8-sig\") \n",
    "    print(\"CSV saved as result2.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65a8dc65-5d89-4402-b2fd-951b0ad1ce41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf4296-1259-4617-98b8-d5bc2e269980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec315c9-71a2-4149-8626-31be317dd742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf320e-b1c7-41a2-8184-7755ee681e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
