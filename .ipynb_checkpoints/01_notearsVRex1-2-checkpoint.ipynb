{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5b5ee2-912a-461f-88c1-a64a7b8cfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## install and import\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63601855-7db7-49bf-b407-fec9a02b131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-igraph\n",
    "from notears.locally_connected import LocallyConnected\n",
    "from notears.lbfgsb_scipy import LBFGSBScipy\n",
    "from notears.trace_expm import trace_expm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import notears.utils as ut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92804479-62b2-49fc-8844-8728aa57c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## class\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd09ec5-e518-4eae-892b-80a1da22818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotearsMLP(nn.Module):\n",
    "    def __init__(self, dims, bias=True):\n",
    "        super(NotearsMLP, self).__init__()\n",
    "        assert len(dims) >= 2\n",
    "        assert dims[-1] == 1\n",
    "        d = dims[0]\n",
    "        self.dims = dims\n",
    "        # fc1: variable splitting for l1\n",
    "        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "        # fc2: local linear layers\n",
    "        layers = []\n",
    "        for l in range(len(dims) - 2):\n",
    "            layers.append(LocallyConnected(d, dims[l + 1], dims[l + 2], bias=bias))\n",
    "        self.fc2 = nn.ModuleList(layers)\n",
    "\n",
    "    def _bounds(self):\n",
    "        d = self.dims[0]\n",
    "        bounds = []\n",
    "        for j in range(d):\n",
    "            for m in range(self.dims[1]):\n",
    "                for i in range(d):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0)\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1]\n",
    "        x = x.view(-1, self.dims[0], self.dims[1])  # [n, d, m1]\n",
    "        for fc in self.fc2:\n",
    "            x = torch.sigmoid(x)  # [n, d, m1]\n",
    "            x = fc(x)  # [n, d, m2]\n",
    "        x = x.squeeze(dim=2)  # [n, d]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        \"\"\"Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(d) + A / d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, d - 1)\n",
    "        # h = (E.t() * M).sum() - d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        \"\"\"Take 2-norm-squared of all parameters\"\"\"\n",
    "        reg = 0.\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        reg += torch.sum(fc1_weight ** 2)\n",
    "        for fc in self.fc2:\n",
    "            reg += torch.sum(fc.weight ** 2)\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        \"\"\"Take l1 norm of fc1 weight\"\"\"\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n",
    "        \"\"\"Get W from fc1 weights, take 2-norm over m1 dim\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W\n",
    "\n",
    "class NotearsSobolev(nn.Module):\n",
    "    def __init__(self, d, k):\n",
    "        \"\"\"d: num variables k: num expansion of each variable\"\"\"\n",
    "        super(NotearsSobolev, self).__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.fc1_pos = nn.Linear(d * k, d, bias=False)  # ik -> j\n",
    "        self.fc1_neg = nn.Linear(d * k, d, bias=False)\n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "        nn.init.zeros_(self.fc1_pos.weight)\n",
    "        nn.init.zeros_(self.fc1_neg.weight)\n",
    "        self.l2_reg_store = None\n",
    "\n",
    "    def _bounds(self):\n",
    "        # weight shape [j, ik]\n",
    "        bounds = []\n",
    "        for j in range(self.d):\n",
    "            for i in range(self.d):\n",
    "                for _ in range(self.k):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0)\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "\n",
    "    def sobolev_basis(self, x):  # [n, d] -> [n, dk]\n",
    "        seq = []\n",
    "        for kk in range(self.k):\n",
    "            mu = 2.0 / (2 * kk + 1) / math.pi  # sobolev basis\n",
    "            psi = mu * torch.sin(x / mu)\n",
    "            seq.append(psi)  # [n, d] * k\n",
    "        bases = torch.stack(seq, dim=2)  # [n, d, k]\n",
    "        bases = bases.view(-1, self.d * self.k)  # [n, dk]\n",
    "        return bases\n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        bases = self.sobolev_basis(x)  # [n, dk]\n",
    "        x = self.fc1_pos(bases) - self.fc1_neg(bases)  # [n, d]\n",
    "        self.l2_reg_store = torch.sum(x ** 2) / x.shape[0]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
    "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(self.d) + A / self.d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, self.d - 1)\n",
    "        # h = (E.t() * M).sum() - self.d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        reg = self.l2_reg_store\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
    "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fed846-a4e8-4eeb-8137-54a2848032bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## function\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bf6c318-b4b1-4c48-a562-c5c7e977e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(output, target):\n",
    "    n = target.shape[0]\n",
    "    loss = 0.5 / n * torch.sum((output - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "def dual_ascent_step(model, X, lambda1, lambda2, rho, alpha, h, rho_max):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian.\"\"\"\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters())\n",
    "    X_torch = torch.from_numpy(X)\n",
    "    while rho < rho_max:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            X_hat = model(X_torch)\n",
    "            loss = squared_loss(X_hat, X_torch)\n",
    "            h_val = model.h_func()\n",
    "            penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            primal_obj = loss + penalty + l2_reg + l1_reg\n",
    "            # primal_obj = loss + penalty\n",
    "            primal_obj.backward()\n",
    "            return primal_obj\n",
    "        optimizer.step(closure)  # NOTE: updates model in-place\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "    alpha += rho * h_new\n",
    "    return rho, alpha, h_new\n",
    "\n",
    "def dual_ascent_step_with_loss_std(model, X_list, lambda1, lambda2, std_lambda, rho, alpha, h, rho_max):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian, with feature-wise error std penalty across datasets.\"\"\"\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters())\n",
    "    X_tensors = [torch.from_numpy(X).float().to(torch.double) for X in X_list]\n",
    "    \n",
    "    while rho < rho_max:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "            individual_losses = []\n",
    "            feature_wise_errors = []  # To store feature-wise errors for each dataset\n",
    "\n",
    "            # Step 1: Calculate the error for each feature for each dataset\n",
    "            for X in X_tensors:\n",
    "                X_hat = model(X)\n",
    "                loss = squared_loss(X_hat, X)\n",
    "                individual_losses.append(loss)\n",
    "                total_loss += loss \n",
    "                \n",
    "                # Calculate the feature-wise errors (dimension will be batch_size x num_features)\n",
    "                feature_errors = 0.5 / X.shape[0] * torch.sum((X_hat - X) ** 2, dim=0)\n",
    "                feature_wise_errors.append(feature_errors)\n",
    "\n",
    "            total_loss = total_loss / len(individual_losses)\n",
    "            # print(total_loss)\n",
    "            \n",
    "            # Step 2: Stack the feature-wise errors across datasets\n",
    "            feature_wise_errors = torch.stack(feature_wise_errors)  # shape: (num_datasets, num_features)\n",
    "            \n",
    "            # Step 3: Calculate the standard deviation across datasets for each feature\n",
    "            feature_std_devs = torch.std(feature_wise_errors, dim=0)  # Std deviation across datasets for each feature\n",
    "            \n",
    "            # Step 4: Sum the feature-wise std deviations and add as a penalty to the total loss\n",
    "            # print(feature_std_devs.shape)\n",
    "            # print(torch.mean(feature_std_devs))           \n",
    "            total_loss += std_lambda * torch.mean(feature_std_devs)\n",
    "\n",
    "            # Existing terms for h penalty and regularization\n",
    "            h_val = model.h_func()\n",
    "            penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "            # print(penalty)\n",
    "            total_loss += penalty\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            # print(l1_reg, l2_reg)            \n",
    "            total_loss += l2_reg + l1_reg\n",
    "\n",
    "            # Backpropagate\n",
    "            total_loss.backward()\n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)  \n",
    "\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    alpha += rho * h_new\n",
    "    return rho, alpha, h_new\n",
    "\n",
    "# def dual_ascent_step_with_loss_std(model, X_list, lambda1, lambda2, std_lambda, rho, alpha, h, rho_max):\n",
    "#     \"\"\"Perform one step of dual ascent in augmented Lagrangian, with loss std penalty.\"\"\"\n",
    "#     h_new = None\n",
    "#     optimizer = LBFGSBScipy(model.parameters())\n",
    "#     X_tensors = [torch.from_numpy(X).float().to(torch.double) for X in X_list]\n",
    "    \n",
    "#     losses = []    \n",
    "#     while rho < rho_max:\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             total_loss = 0\n",
    "#             individual_losses = []\n",
    "#             for X in X_tensors:\n",
    "#                 X_hat = model(X)\n",
    "#                 loss = squared_loss(X_hat, X)\n",
    "#                 individual_losses.append(loss)\n",
    "#                 total_loss += loss \n",
    "#             total_loss = total_loss / len(individual_losses)\n",
    "#             loss_std = torch.std(torch.stack(individual_losses))\n",
    "#             total_loss += std_lambda * loss_std \n",
    "#             h_val = model.h_func()\n",
    "#             penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "#             total_loss += penalty\n",
    "#             l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "#             l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "#             total_loss += l2_reg + l1_reg\n",
    "#             total_loss.backward()\n",
    "#             return total_loss\n",
    "#         optimizer.step(closure)  \n",
    "#         with torch.no_grad():\n",
    "#             h_new = model.h_func().item()\n",
    "#         if h_new > 0.25 * h:\n",
    "#             rho *= 10\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "#     alpha += rho * h_new\n",
    "#     return rho, alpha, h_new\n",
    "\n",
    "def notears_nonlinear(model: nn.Module,\n",
    "                      X: np.ndarray,\n",
    "                      lambda1: float = 0.,\n",
    "                      lambda2: float = 0.,\n",
    "                      max_iter: int = 100,\n",
    "                      h_tol: float = 1e-8,\n",
    "                      rho_max: float = 1e+16,\n",
    "                      w_threshold: float = 0.15):\n",
    "    rho, alpha, h = 1.0, 0.0, np.inf\n",
    "    for _ in range(max_iter):\n",
    "        rho, alpha, h = dual_ascent_step(model, X, lambda1, lambda2,\n",
    "                                         rho, alpha, h, rho_max)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = model.fc1_to_adj()\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n",
    "\n",
    "def notears_nonlinear_with_loss_std(model: nn.Module,\n",
    "                                    X_list: list,  \n",
    "                                    lambda1: float = 0.0,\n",
    "                                    lambda2: float = 0.0,\n",
    "                                    max_iter: int = 100,\n",
    "                                    h_tol: float = 1e-8,\n",
    "                                    rho_max: float = 1e+16,\n",
    "                                    w_threshold: float = 0.15,\n",
    "                                    std_lambda: float = 1.0):  \n",
    "    rho, alpha, h = 1.0, 0.0, np.inf\n",
    "    for _ in range(max_iter):\n",
    "        rho, alpha, h = dual_ascent_step_with_loss_std(\n",
    "            model, X_list, lambda1, lambda2, std_lambda, rho, alpha, h, rho_max)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = model.fc1_to_adj()\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e92c3bdf-b410-456b-bd61-45d007ae18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## initialize\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd5a91f6-6052-48ef-9d89-d9bf4fd2a2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3)\n",
    "ut.set_random_seed(123)\n",
    "n, d, s0, graph_type, sem_type = 200, 10, 20, 'ER', 'mlp'\n",
    "B_true = ut.simulate_dag(d, s0, graph_type)\n",
    "np.savetxt('inputs/W_true.csv', B_true, delimiter=',')\n",
    "# noise_scale = np.ones(d)\n",
    "# X = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale)\n",
    "# np.savetxt('X.csv', X, delimiter=',')\n",
    "noise_scales = [0.2, 1, 2, 5, 10]\n",
    "for i, noise_scale_value in enumerate(noise_scales):\n",
    "    noise_scale = np.full(d, noise_scale_value)  \n",
    "    X = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale)\n",
    "    np.savetxt(f'inputs/X_{i}.csv', X, delimiter=',')  \n",
    "    \n",
    "X_0 = np.loadtxt('inputs/X_0.csv', delimiter=',')\n",
    "X_1 = np.loadtxt('inputs/X_1.csv', delimiter=',')\n",
    "X_2 = np.loadtxt('inputs/X_2.csv', delimiter=',')\n",
    "X_3 = np.loadtxt('inputs/X_3.csv', delimiter=',')\n",
    "X_4 = np.loadtxt('inputs/X_4.csv', delimiter=',')\n",
    "X_list = [X_0, X_1, X_2, X_3, X_4]  # List of datasets\n",
    "scaler = StandardScaler()\n",
    "X_list_standardized = [scaler.fit_transform(X) for X in X_list]\n",
    "X_combined = np.vstack([X for X in X_list_standardized]) \n",
    "print(X_combined.shape)\n",
    "noise_scale_value_test = np.random.uniform(min(noise_scales), max(noise_scales))\n",
    "noise_scale_test = np.full(d, noise_scale_value_test)\n",
    "X_test = ut.simulate_nonlinear_sem(B_true, n, sem_type, noise_scale_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "189046e5-1e6a-4c90-885a-8929095a4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## NOTEARS\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f0ab1-6427-4b07-b3b8-0d4ff758f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fdr, list_shd, list_tpr, list_nnz = [], [], [], []\n",
    "for i in range(5):\n",
    "    model = NotearsMLP(dims=[d, 10, 1], bias=True)\n",
    "    W_est = notears_nonlinear(model, X_combined, lambda1=0.01, lambda2=0.01)\n",
    "    assert ut.is_dag(W_est)\n",
    "    np.savetxt('outputs/W_est.csv', W_est, delimiter=',')\n",
    "    acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "    print(i, acc)\n",
    "    list_fdr.append(acc['fdr'])\n",
    "    list_shd.append(acc['shd'])\n",
    "    list_tpr.append(acc['tpr'])\n",
    "    list_nnz.append(acc['nnz'])    \n",
    "print()\n",
    "print()\n",
    "print(f'FDR: {np.mean(list_fdr):.4f} ± {np.std(list_fdr):.4f}')\n",
    "print(f'SHD: {np.mean(list_shd):.4f} ± {np.std(list_shd):.4f}')\n",
    "print(f'TPR: {np.mean(list_tpr):.4f} ± {np.std(list_tpr):.4f}')\n",
    "print(f'NNZ: {np.mean(list_nnz):.4f} ± {np.std(list_nnz):.4f}')\n",
    "X_hat_test = model(torch.from_numpy(X_test).float().to(torch.double))\n",
    "loss_test = squared_loss(X_hat_test, torch.from_numpy(X_test).float().to(torch.double))\n",
    "print(f'Test squared loss on original values: {loss_test.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a5b836-d9d5-4284-817c-6a6ee08241e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## invariant NOTEARS\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924960a-73b2-44ea-ba41-f496a95f0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fdr, list_shd, list_tpr, list_nnz = [], [], [], []\n",
    "for i in range(5):\n",
    "    model = NotearsMLP(dims=[d, 10, 1], bias=True)\n",
    "    W_est = notears_nonlinear_with_loss_std(model, X_list_standardized, lambda1=0.01, lambda2=0.01, std_lambda=1)\n",
    "    assert ut.is_dag(W_est)\n",
    "    np.savetxt('outputs/W_est2.csv', W_est, delimiter=',')\n",
    "    acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "    print(i, acc)\n",
    "    list_fdr.append(acc['fdr'])\n",
    "    list_shd.append(acc['shd'])\n",
    "    list_tpr.append(acc['tpr'])\n",
    "    list_nnz.append(acc['nnz'])        \n",
    "print()\n",
    "print()\n",
    "print(f'FDR: {np.mean(list_fdr):.4f} ± {np.std(list_fdr):.4f}')\n",
    "print(f'SHD: {np.mean(list_shd):.4f} ± {np.std(list_shd):.4f}')\n",
    "print(f'TPR: {np.mean(list_tpr):.4f} ± {np.std(list_tpr):.4f}')\n",
    "print(f'NNZ: {np.mean(list_nnz):.4f} ± {np.std(list_nnz):.4f}')\n",
    "X_hat_test = model(torch.from_numpy(X_test).float().to(torch.double))\n",
    "loss_test = squared_loss(X_hat_test, torch.from_numpy(X_test).float().to(torch.double))\n",
    "print(f'Test squared loss on original values: {loss_test.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63cb7d2-1f78-4e17-87a7-678f85ab5a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf4296-1259-4617-98b8-d5bc2e269980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
